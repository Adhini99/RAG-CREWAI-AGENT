{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPSfjuCGoz9A"
      },
      "source": [
        "# **Agentic RAG Using CrewAI & LangChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FpD8AETpClM",
        "outputId": "7181df41-b924-4877-a3e7-cf149f1a43cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfbPZS5zpRco"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade crewai==0.28.8 crewai_tools==0.1.6 langchain_community==0.0.29 sentence-transformers langchain-groq --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtXGCDKcw41L"
      },
      "outputs": [],
      "source": [
        "# Install a compatible version of numpy\n",
        "!pip install numpy==1.26.4 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f5c8262"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb==0.4.24 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_nFs7SvwivI",
        "outputId": "45627a80-e17c-46ca-a720-7b1f47ea7ea5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:918: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `CrewAgentExecutor` to V2.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "from crewai_tools import PDFSearchTool\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from crewai_tools  import tool\n",
        "from crewai import Crew\n",
        "from crewai import Task\n",
        "from crewai import Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCzM6wwWxXIs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the API key\n",
        "os.environ['GROQ_API_KEY'] = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Epp4qm2x66c"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    openai_api_base=\"https://api.groq.com/openai/v1\",\n",
        "    openai_api_key=os.environ['GROQ_API_KEY'],\n",
        "    model_name=\"llama3-8b-8192\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmOkfOBNyDfF"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    openai_api_base=\"https://api.groq.com/openai/v1\",\n",
        "    openai_api_key=os.environ['GROQ_API_KEY'],\n",
        "    model_name=\"llama3-8b-8192\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2cFWztzyYiH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "pdf_url = 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'\n",
        "response = requests.get(pdf_url)\n",
        "\n",
        "with open('attenstion_is_all_you_need.pdf', 'wb') as file:\n",
        "    file.write(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNLHq2_UzQBb"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_huggingface langchain-core==0.1.53 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "cdd4fc4eb2c64090868ac9effe2843aa",
            "7c5da6920a2848679ea70ec4f6dee3f4",
            "8229e26f67ab4b8dac08d4fed383e812",
            "6e640cfc2b064b48ab91cda5a5c8c8c1",
            "f304b3d7f8e64784af92e99a3b437b54",
            "ab2290afa97c4a4e817f8ab928442204",
            "e1696d73397a4992bdfc013d18d5c2d0",
            "88d3b2f4311e42418d47a5e7681f9926",
            "6a779c5455d842759ccdfb5767fbafed",
            "2438fec919d74b7bab0e998553343dc9",
            "6a40dd8858db4756ba0176ff51bf5629",
            "a6ddf1c114334e1f9bdf1b7feb3baae2",
            "a683056f96c5452fb94a4e41f0401e63",
            "d46ed19d45124fb2a84f065a783ac2ac",
            "29de276a1b014357bd063f1785747e46",
            "8bc8386d4ce04f628cad98b7ca1582ab",
            "daa06c7413d340e8935024e9cb747d14",
            "de023d1cc1e24bd9a63e289225df2729",
            "b4e63e9e63a14238b9e452ea0ba94a47",
            "bf3b0d9ab2974d2583e893ed50cbf880",
            "ae5642c1bb00441ea62cde365bdb4bc1",
            "1f65f2fde8c44260b4841c06f0f6aa27",
            "528103f63c11499b801cae916e94e08d",
            "c82050c17dd945dca5e55f09a7ff370b",
            "3dc5e10aa5454133950b30f9de5bc4f1",
            "6f0d685710c74f839732d67b0aa83101",
            "f9fd06ebabd34ec9afd99ad6c1f18bb8",
            "897eb7d5f8384f6987ede3b704444526",
            "04ed34ef85714be9b1ea0d0ef5609307",
            "0128a1409279403380b56c2fcf9e6b3d",
            "0982a39041864c918fb2e53d4e4eed9c",
            "8f077f4c82c541fdad48218483381fc3",
            "1f59e863e66b4123b40d122d56a79e59",
            "0525060f77bb4a86b203c4f734ae7514",
            "d9b0a2d3bfe9491086bd9a27ad8bf0ca",
            "874ce4d74a2d43e0802c33aa299b869c",
            "4a510dc710c144078487f8364b8da607",
            "7dabe8903cca47ff85a0c80b42805959",
            "b1f47123d6704e80bbc87ae5c05b8304",
            "8853fd1d51624303a747cdd0843cb6c1",
            "5312837f7c3f456ea871b39939e542b4",
            "d4bcf8943bb34d8a9403548ebff58d12",
            "1a5f3b8988a24f91a2578dead195ee71",
            "bead3fd07582495081888b97050abf81",
            "2ae7b3732ea644cc9df927b5d959ccdc",
            "06d2a9b91d5c4041afe2803cf2130fe4",
            "22ccf6291cfa431d98f033d0ce435ea4",
            "0e24f115576f4a9d854b52235b7d1100",
            "23ab8983b9a141218fece2b98997f080",
            "e231320e3d22411cb844e2f69e84f7c2",
            "7227f155638e4e408123543600c0d8e0",
            "e3f8b5143e8541ff968e418779b9534e",
            "cba8850fda6e4f50ae09dbb0c09b8851",
            "7ca5c187aff445f782f1ecebdb949b4c",
            "9e1ff8af6625440988d5d2f666da124e",
            "c935e4fbac764774aee404f844b7c1d5",
            "7fb162ec6a844167962c7280e84756b6",
            "ee0b4d7898ef4bb697b44d000ebb5fd7",
            "87a45fbe66d44c22ab3c96cfa107870d",
            "54b11857f50149cfabb02739ae4d7411",
            "2b3e5cfc9d3941bb8cbcf1c60c0639a5",
            "45d76c07213f44ee908ca7f8d568f3fb",
            "cc89f129fd4d410ab3686eb9b96adb11",
            "c5ffd4c355494bd686a713cf46e0c0f2",
            "b0602b8a19aa4a70952c7559136e86a9",
            "e44cec9420dc40d0866ec7216f2e0e5f",
            "b581dad444fe4a63a7a674ab47c0e826",
            "36abad11fbd844b48e636be8cd3b55f8",
            "522e77b34dae4521b7679cd2eafcb39b",
            "812eea4e4dbe4000b82dc3c6641b5a73",
            "51e00c81c4a64e1ab97380880d660bc0",
            "c9095db6d7dc42e09a46bc54d491f3b4",
            "491cdfb8894b469487dd4a9511bf3692",
            "2f9761e9909447d6bf6f696d50c1cf71",
            "f3e9de2fac82416ca2ebc468d30882ae",
            "6a436c3307fc409893697bf95470e2ad",
            "5e5b69528d90462cb4cfd76ecbbeccc6",
            "b3721d2aeaba4f34a5302f8b0557ffba",
            "f5d76a566374430288e02b528095cac3",
            "11f607cda74444a99cd24e9bcc569d5b",
            "673ee16e85fa4d6cbf519d6baaffff16",
            "1cf787e298e444a4a03260b99421dd8a",
            "3ba90d833ffc42f0891749f5de251de6",
            "5ec27e38471e4411aeb6bb3864d7339a",
            "0dd08f40bbd240eeb26bb4f1f7d95953",
            "5b541ae03c644cb9954e5b94b898708e",
            "408ce81d723c4912b98a94fa28c8c00a",
            "d293045b68ef4c309ee7029f870e57b4",
            "a4c53d6aaf43422e8eb8540c421bc3a9",
            "e41f9801bf56479f8cfa4ed9be9ef88f",
            "adb5c749685a4d50be3cf2bb22128834",
            "719a1fe494864ff3979ac996a8e621e4",
            "c6ebf6d3924f42a1b8d41f92039443cf",
            "5bb0396eb0be4b9bbcdd7ebb86b6bbf6",
            "4958f60ebba5459986ce18f5693e8e8d",
            "231c4983cba64f02bd8bd71c153b16ac",
            "d9014aed7d054555ab8551ba015c7d57",
            "5212e027c9ec43bf85cff3731c2232f2",
            "2ab9fae4cc624dc6b59e5d3f094e8240",
            "acf1e401fa464763820ed27bb37c9e67",
            "2c9a907d37f644228b6a358c11d9730c",
            "7acfd96f99a34c2c9932629fbcd3c279",
            "23ad88c54efc4ec59e91e187b0108fea",
            "03b73beb36964d7e94137f6c7f1e9bde",
            "413f645d3e9f438da201f081b017d212",
            "c4dd7245a9854a0d99bd18e84e02455e",
            "416f584aabab4bbcb52c32c79b0518a8",
            "a34c09d3460e48cb8d822f10d7111f56",
            "1e17d321123249839c4cf67c4d62bef4",
            "fbc1e4d4f3554510bf037cf9a741baba",
            "69decfc9528d41489d5cde1c37a98445",
            "92bd9de3d40440ed94957d646a0cabbf",
            "87219cd862034305903a4530c7fb847b",
            "d72a6036f5b34e74b9074b4e2d05f071",
            "c86d1c424ee944c4a969def772aced42",
            "13665a13362a45c3be8eac43fc647252",
            "c300296649e24fb2ad93aa563e4ee7e4",
            "baf9ea586b894122962d984ec5facf85",
            "61cee338a8e0416db158abf9ec6ae59f",
            "6be655ccd73246038f6ce632cd67901d",
            "615b7a70286e41519031db0fec9c52dd"
          ]
        },
        "id": "2GXxsxR0yknb",
        "outputId": "547f8c43-4023-41bb-ef0d-8ec72b13d7e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdd4fc4eb2c64090868ac9effe2843aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6ddf1c114334e1f9bdf1b7feb3baae2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "528103f63c11499b801cae916e94e08d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0525060f77bb4a86b203c4f734ae7514",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ae7b3732ea644cc9df927b5d959ccdc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c935e4fbac764774aee404f844b7c1d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b581dad444fe4a63a7a674ab47c0e826",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3721d2aeaba4f34a5302f8b0557ffba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4c53d6aaf43422e8eb8540c421bc3a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "acf1e401fa464763820ed27bb37c9e67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69decfc9528d41489d5cde1c37a98445",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inserting batches in chromadb: 100%|██████████| 1/1 [00:15<00:00, 15.49s/it]\n"
          ]
        }
      ],
      "source": [
        "rag_tool = PDFSearchTool(pdf='attenstion_is_all_you_need.pdf',\n",
        "    config=dict(\n",
        "        llm=dict(\n",
        "            provider=\"groq\", # or google, openai, anthropic, llama2, ...\n",
        "            config=dict(\n",
        "                model=\"llama3-8b-8192\",\n",
        "                # temperature=0.5,\n",
        "                # top_p=1,\n",
        "                # stream=true,\n",
        "            ),\n",
        "        ),\n",
        "        embedder=dict(\n",
        "            provider=\"huggingface\", # or openai, ollama, ...\n",
        "            config=dict(\n",
        "                model=\"BAAI/bge-small-en-v1.5\",\n",
        "                #task_type=\"retrieval_document\",\n",
        "                # title=\"Embeddings\",\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "r-yJFWC30vw-",
        "outputId": "f4323644-8806-49f3-8bd4-4bc6348a9e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Tool: Search a PDF's content\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Relevant Content:\\nThis makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer\\n\\nIn terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6\\n\\nAttention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.comAidan N. Gomez∗† University of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_tool.run(\"How did self-attention mechanism evolve in large language models?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okofk0MsHLJa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the Tavily API key\n",
        "os.environ['TAVILY_API_KEY'] = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4mLgIloIKbT"
      },
      "outputs": [],
      "source": [
        "web_search_tool = TavilySearchResults(k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akRQI_SeKRkp",
        "outputId": "e3b0a9ab-b2a6-47ff-81f0-a2b3550caece"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'url': 'https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention',\n",
              "  'content': \"Copy link Facebook Email Notes More\\n\\n[41](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention/comments)13\\n\\n[Share](javascript:void(0))\\n\\nThis article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama. Self-attention and related mechanisms are core components of LLMs, making them a useful topic to understand when working with these models. [...] We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the input's context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document.\"},\n",
              " {'url': 'https://medium.com/@smrati.katiyar/explaining-self-attention-in-llms-b41763e54a11',\n",
              "  'content': 'Self-attention is a way for models to learn which parts of the input are important for understanding a particular word or token. It’s especially powerful in large language models because it enables them to grasp complex relationships between words, regardless of their position in the sentence.\\n\\n--\\n\\n--\\n\\n![smrati katiyar](https://miro.medium.com/v2/resize:fill:96:96/1*Q9uVBJn7-4Lbn9oH57npsw.jpeg)\\n![smrati katiyar](https://miro.medium.com/v2/resize:fill:128:128/1*Q9uVBJn7-4Lbn9oH57npsw.jpeg) [...] Self-attention is a fundamental concept in large language models (LLMs), like GPT or BERT, that helps the model focus on different parts of the input sequence to understand the meaning of each word or token better. Let’s break this down from the basics with an example.\\n\\n# Basics of Attention\\n\\nIn natural language, the meaning of a word often depends on the context around it. For example, consider the sentence:\\n\\n**“The cat chased the mouse because it was hungry.”** [...] **Self-attention** allows a model to look at all the words in a sentence and decide which words are important to understand each individual word. Unlike older models that process a word in isolation, self-attention lets a word attend to, or focus on, other words in the sentence.\\n\\n## Steps of Self-Attention:\\n\\nFor example, in our sentence:'},\n",
              " {'url': 'https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html',\n",
              "  'content': 'We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the input’s context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document. [...] In this article, we saw how self-attention works using a step-by-step coding approach. We then extended this concept to multi-head attention, the widely used component of large-language transformers. After discussing self-attention and multi-head attention, we introduced yet another concept: cross-attention, which is a flavor of self-attention that we can apply between two different sequences. This is already a lot of information to take in. Let’s leave the training of a neural network using [...] To overcome this issue, attention mechanisms were introduced to give access to all sequence elements at each time step. The key is to be selective and determine which words are most important in a specific context. In 2017, the transformer architecture introduced a standalone self-attention mechanism, eliminating the need for RNNs altogether.'},\n",
              " {'url': 'https://www.ibm.com/think/topics/self-attention',\n",
              "  'content': 'Self-attention is a type of [attention mechanism](https://www.ibm.com/think/topics/attention-mechanism) used in machine learning models. This mechanism is used to weigh the importance of tokens or words in an input sequence to better understand the relations between them.It is a crucial part of transformer models, a powerful artificial intelligence architecture that is essential for [natural language processing (NLP)](https://www.ibm.com/think/topics/natural-language-processing) tasks. The [...] **NLP tasks:**The self-attention mechanism enhances the linguistic capabilities of machine learning models by allowing the efficient and complete analysis of an entire text. Research has shown advancements in sentiment classification.[6](https://www.ibm.com/think/topics/self-attention#f06) Models can perform NLP tasks well because the attention layer allows it to compute the relation between words regardless of the distance between them.[7](https://www.ibm.com/think/topics/self-attention#f07) [...] Self-attention played a key role in the advancement of LLMs by enabling parallelization within training examples. This method is useful because the longer the sequence length, the more memory constraints limit batching across training examples. Using self-attention, LLM training data can be split into batches and processed concurrently on multiple GPUs.[1](https://www.ibm.com/think/topics/self-attention#f01) Self-attention reduces the computational power needed to train machine learning models'},\n",
              " {'url': 'https://www.linkedin.com/pulse/what-self-attention-impact-large-language-models-llm-nikhil-goel-srpbc',\n",
              "  'content': \"![Image 10: Article content](https://www.linkedin.com/pulse/what-self-attention-impact-large-language-models-llm-nikhil-goel-srpbc)\\n\\nDecoder Only Transformer Model for Text Generation\\n\\nIn summary Self-Attention or Attention Mechanism is KEY if you want to understand the working of LLM's.\\n\\nHope you all have a good read.\\n\\nThanks.\\n\\nDisclaimer: Opinion / Views expressed above are the author's personal and has no bearing or affiliation to the authors current employer or any earlier/past employers. [...] Why Self Attention? For Machines / Neural Nets to correctly understand or model the language / large text sequences, the important part for machines is to learn the relationship or one word with another word in a text sequence.\\n\\nWhat is Self-Attention? This learning of context or relationship or one word with another is enabled by Self-Attention mechanism. Through Self Attention Neural Nets understand the language context.\\n\\nHow Self Attention Works Mathematically? [...] The way Attention is used in LLM (Decoder Only Models) is as depicted in below image. During pre-training the whole text sequence is provided to the Decoder only models and Multi Head Self Attention blocks ('n' number of blocks) are used to encode the text and then passed thru linear layers to get the probability of the token to be generated. (This is more or less the top-level flow of an LLM, but still lot of nuances remain to be explained by me.)\"}]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "web_search_tool.run(\"What is self-attention mechansim in large language models?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxd8Ia5ELJAx"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def router_tool(question):\n",
        "  \"\"\"Router Function\"\"\"\n",
        "  if 'self-attention' in question:\n",
        "    return 'vectorstore'\n",
        "  else:\n",
        "    return 'web_search'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xev1agcmLQrn"
      },
      "outputs": [],
      "source": [
        "Router_Agent = Agent(\n",
        "  role='Router',\n",
        "  goal='Route user question to a vectorstore or web search',\n",
        "  backstory=(\n",
        "    \"You are an expert at routing a user question to a vectorstore or web search.\"\n",
        "    \"Use the vectorstore for questions on concept related to Retrieval-Augmented Generation.\"\n",
        "    \"You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search.\"\n",
        "  ),\n",
        "  verbose=True,\n",
        "  allow_delegation=False,\n",
        "  llm=llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66L8PzUOLgSE"
      },
      "outputs": [],
      "source": [
        "Retriever_Agent = Agent(\n",
        "role=\"Retriever\",\n",
        "goal=\"Use the information retrieved from the vectorstore to answer the question\",\n",
        "backstory=(\n",
        "    \"You are an assistant for question-answering tasks.\"\n",
        "    \"Use the information present in the retrieved context to answer the question.\"\n",
        "    \"You have to provide a clear concise answer.\"\n",
        "),\n",
        "verbose=True,\n",
        "allow_delegation=False,\n",
        "llm=llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7Z2ve1qLryO"
      },
      "outputs": [],
      "source": [
        "Grader_agent =  Agent(\n",
        "  role='Answer Grader',\n",
        "  goal='Filter out erroneous retrievals',\n",
        "  backstory=(\n",
        "    \"You are a grader assessing relevance of a retrieved document to a user question.\"\n",
        "    \"If the document contains keywords related to the user question, grade it as relevant.\"\n",
        "    \"It does not need to be a stringent test.You have to make sure that the answer is relevant to the question.\"\n",
        "  ),\n",
        "  verbose=True,\n",
        "  allow_delegation=False,\n",
        "  llm=llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCj8SWI0LxxB"
      },
      "outputs": [],
      "source": [
        "hallucination_grader = Agent(\n",
        "    role=\"Hallucination Grader\",\n",
        "    goal=\"Filter out hallucination\",\n",
        "    backstory=(\n",
        "        \"You are a hallucination grader assessing whether an answer is grounded in / supported by a set of facts.\"\n",
        "        \"Make sure you meticulously review the answer and check if the response provided is in alignmnet with the question asked\"\n",
        "    ),\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akQ-ysD8L9vd"
      },
      "outputs": [],
      "source": [
        "answer_grader = Agent(\n",
        "    role=\"Answer Grader\",\n",
        "    goal=\"Filter out hallucination from the answer.\",\n",
        "    backstory=(\n",
        "        \"You are a grader assessing whether an answer is useful to resolve a question.\"\n",
        "        \"Make sure you meticulously review the answer and check if it makes sense for the question asked\"\n",
        "        \"If the answer is relevant generate a clear and concise response.\"\n",
        "        \"If the answer gnerated is not relevant then perform a websearch using 'web_search_tool'\"\n",
        "    ),\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVW_TC3QMKMD"
      },
      "source": [
        "\n",
        "\n",
        "# Defining Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1iSO3OgMNbw"
      },
      "outputs": [],
      "source": [
        "router_task = Task(\n",
        "    description=(\"Analyse the keywords in the question {question}\"\n",
        "    \"Based on the keywords decide whether it is eligible for a vectorstore search or a web search.\"\n",
        "    \"Return a single word 'vectorstore' if it is eligible for vectorstore search.\"\n",
        "    \"Return a single word 'websearch' if it is eligible for web search.\"\n",
        "    \"Do not provide any other premable or explaination.\"\n",
        "    ),\n",
        "    expected_output=(\"Give a binary choice 'websearch' or 'vectorstore' based on the question\"\n",
        "    \"Do not provide any other premable or explaination.\"),\n",
        "    agent=Router_Agent,\n",
        "    tools=[router_tool],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxzl2UhxMa9w"
      },
      "outputs": [],
      "source": [
        "retriever_task = Task(\n",
        "    description=(\"Based on the response from the router task extract information for the question {question} with the help of the respective tool.\"\n",
        "    \"Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.\"\n",
        "    \"Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\"\n",
        "    ),\n",
        "    expected_output=(\"You should analyse the output of the 'router_task'\"\n",
        "    \"If the response is 'websearch' then use the web_search_tool to retrieve information from the web.\"\n",
        "    \"If the response is 'vectorstore' then use the rag_tool to retrieve information from the vectorstore.\"\n",
        "    \"Return a claer and consise text as response.\"),\n",
        "    agent=Retriever_Agent,\n",
        "    context=[router_task],\n",
        "   #tools=[retriever_tool],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL0K8Aa2MgbN"
      },
      "outputs": [],
      "source": [
        "grader_task = Task(\n",
        "    description=(\"Based on the response from the retriever task for the quetion {question} evaluate whether the retrieved content is relevant to the question.\"\n",
        "    ),\n",
        "    expected_output=(\"Binary score 'yes' or 'no' score to indicate whether the document is relevant to the question\"\n",
        "    \"You must answer 'yes' if the response from the 'retriever_task' is in alignment with the question asked.\"\n",
        "    \"You must answer 'no' if the response from the 'retriever_task' is not in alignment with the question asked.\"\n",
        "    \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"),\n",
        "    agent=Grader_agent,\n",
        "    context=[retriever_task],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_-h3OCJMkAe"
      },
      "outputs": [],
      "source": [
        "hallucination_task = Task(\n",
        "    description=(\"Based on the response from the grader task for the quetion {question} evaluate whether the answer is grounded in / supported by a set of facts.\"),\n",
        "    expected_output=(\"Binary score 'yes' or 'no' score to indicate whether the answer is sync with the question asked\"\n",
        "    \"Respond 'yes' if the answer is in useful and contains fact about the question asked.\"\n",
        "    \"Respond 'no' if the answer is not useful and does not contains fact about the question asked.\"\n",
        "    \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"),\n",
        "    agent=hallucination_grader,\n",
        "    context=[grader_task],\n",
        ")\n",
        "\n",
        "answer_task = Task(\n",
        "    description=(\"Based on the response from the hallucination task for the quetion {question} evaluate whether the answer is useful to resolve the question.\"\n",
        "    \"If the answer is 'yes' return a clear and concise answer.\"\n",
        "    \"If the answer is 'no' then perform a 'websearch' and return the response\"),\n",
        "    expected_output=(\"Return a clear and concise response if the response from 'hallucination_task' is 'yes'.\"\n",
        "    \"Perform a web search using 'web_search_tool' and return ta clear and concise response only if the response from 'hallucination_task' is 'no'.\"\n",
        "    \"Otherwise respond as 'Sorry! unable to find a valid response'.\"),\n",
        "    context=[hallucination_task],\n",
        "    agent=answer_grader,\n",
        "    #tools=[answer_grader_tool],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZjQxDcaM35N"
      },
      "source": [
        "# Define a flow for the use case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZwcvX1YM5ig"
      },
      "outputs": [],
      "source": [
        "rag_crew = Crew(\n",
        "    agents=[Router_Agent, Retriever_Agent, Grader_agent, hallucination_grader, answer_grader],\n",
        "    tasks=[router_task, retriever_task, grader_task, hallucination_task, answer_task],\n",
        "    verbose=True,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wobixhG6NFwX"
      },
      "source": [
        "\n",
        "# Ask any query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J7qIPLpNG9M"
      },
      "outputs": [],
      "source": [
        "inputs ={\"question\":\"How does self-attention mechanism help large language models?\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlq3OLiWNUG4"
      },
      "source": [
        "# Kick off the agent pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iggGMh3HNVJ-",
        "outputId": "36384026-ee85-4ba4-fbb5-401a0f3c27ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Router\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Analyse the keywords in the question How does self-attention mechanism help large language models?Based on the keywords decide whether it is eligible for a vectorstore search or a web search.Return a single word 'vectorstore' if it is eligible for vectorstore search.Return a single word 'websearch' if it is eligible for web search.Do not provide any other premable or explaination.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The question seems to be related to the concept of self-attention mechanism in large language models, which is a topic in the field of Retrieval-Augmented Generation.\n",
            "Action: router_tool\n",
            "Action Input: {\"question\": \"How does self-attention mechanism help large language models?\"}\u001b[0m\u001b[95m \n",
            "\n",
            "vectorstore\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mFinal Answer: vectorstore\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Router] Task output: vectorstore\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Retriever\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the router task extract information for the question How does self-attention mechanism help large language models? with the help of the respective tool.Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now have the context and the task to extract information for the question \"How does self-attention mechanism help large language models?\" using the vectorstore.\n",
            "\n",
            "Final Answer: \n",
            "\n",
            "The self-attention mechanism in large language models helps in several ways:\n",
            "\n",
            "* It allows the model to focus on specific parts of the input sequence that are relevant to the task at hand, rather than treating the entire sequence equally. This is particularly important for long-range dependencies, where the model needs to consider the relationships between distant parts of the sequence.\n",
            "* It enables the model to capture complex contextual relationships between different parts of the input sequence, which is essential for tasks such as language translation, question answering, and text summarization.\n",
            "* It helps the model to learn more accurate and nuanced representations of the input sequence, which can lead to better performance on a wide range of tasks.\n",
            "* It allows the model to handle out-of-vocabulary words and rare words more effectively, by focusing on the context in which they appear.\n",
            "\n",
            "These benefits are achieved through the use of attention weights, which are learned during training and represent the importance of each part of the input sequence for the task at hand. The attention weights are then used to compute a weighted sum of the input sequence, which is used as the input to the next layer in the model.\n",
            "\n",
            "Overall, the self-attention mechanism is a key component of large language models, and it plays a critical role in their ability to capture complex contextual relationships and perform well on a wide range of tasks.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Retriever] Task output: The self-attention mechanism in large language models helps in several ways:\n",
            "\n",
            "* It allows the model to focus on specific parts of the input sequence that are relevant to the task at hand, rather than treating the entire sequence equally. This is particularly important for long-range dependencies, where the model needs to consider the relationships between distant parts of the sequence.\n",
            "* It enables the model to capture complex contextual relationships between different parts of the input sequence, which is essential for tasks such as language translation, question answering, and text summarization.\n",
            "* It helps the model to learn more accurate and nuanced representations of the input sequence, which can lead to better performance on a wide range of tasks.\n",
            "* It allows the model to handle out-of-vocabulary words and rare words more effectively, by focusing on the context in which they appear.\n",
            "\n",
            "These benefits are achieved through the use of attention weights, which are learned during training and represent the importance of each part of the input sequence for the task at hand. The attention weights are then used to compute a weighted sum of the input sequence, which is used as the input to the next layer in the model.\n",
            "\n",
            "Overall, the self-attention mechanism is a key component of large language models, and it plays a critical role in their ability to capture complex contextual relationships and perform well on a wide range of tasks.\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the retriever task for the quetion How does self-attention mechanism help large language models? evaluate whether the retrieved content is relevant to the question.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now have a great understanding of the question and the context. I will carefully evaluate the retrieved content to determine its relevance to the question.\n",
            "\n",
            "Final Answer: yes\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: yes\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Hallucination Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the grader task for the quetion How does self-attention mechanism help large language models? evaluate whether the answer is grounded in / supported by a set of facts.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "Final Answer: yes\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Hallucination Grader] Task output: yes\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the hallucination task for the quetion How does self-attention mechanism help large language models? evaluate whether the answer is useful to resolve the question.If the answer is 'yes' return a clear and concise answer.If the answer is 'no' then perform a 'websearch' and return the response\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "\n",
            "Action: Since the response from the hallucination task is \"yes\", I will return a clear and concise answer.\n",
            "\n",
            "Final Answer: The self-attention mechanism in large language models helps by allowing the model to focus on specific parts of the input sequence and weigh their importance. This is particularly useful for tasks that require understanding complex relationships between different parts of the input, such as machine translation and text summarization. By attending to specific parts of the input, the model can better capture the nuances of the input and generate more accurate and relevant outputs.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: The self-attention mechanism in large language models helps by allowing the model to focus on specific parts of the input sequence and weigh their importance. This is particularly useful for tasks that require understanding complex relationships between different parts of the input, such as machine translation and text summarization. By attending to specific parts of the input, the model can better capture the nuances of the input and generate more accurate and relevant outputs.\n",
            "\n",
            "\u001b[00m\n"
          ]
        }
      ],
      "source": [
        "result = rag_crew.kickoff(inputs=inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY2DISxvNeOK",
        "outputId": "8bc52495-e69c-4855-aa1b-11f96fe0f94f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The self-attention mechanism in large language models helps by allowing the model to focus on specific parts of the input sequence and weigh their importance. This is particularly useful for tasks that require understanding complex relationships between different parts of the input, such as machine translation and text summarization. By attending to specific parts of the input, the model can better capture the nuances of the input and generate more accurate and relevant outputs.\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e802207"
      },
      "source": [
        "# Task\n",
        "Create a Streamlit application that takes a user question as input, processes it using a CrewAI agent, and displays the final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ad25fa9"
      },
      "source": [
        "## Extract and refactor code\n",
        "\n",
        "### Subtask:\n",
        "Identify the essential code from the notebook needed for the Streamlit app and refactor it into functions or a class for better organization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkifnJvcQdRL",
        "outputId": "d3a678ba-ade6-431d-8694-eaa2df13b1ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "# prompt: Create a Streamlit application that takes a user question as input, processes it using a CrewAI agent, and displays the final answer.\n",
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive') # This is not needed in a typical Streamlit app deployment\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "from crewai_tools import PDFSearchTool\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from crewai_tools  import tool\n",
        "from crewai import Crew\n",
        "from crewai import Task\n",
        "from crewai import Agent\n",
        "import requests\n",
        "import tempfile\n",
        "\n",
        "# Install necessary libraries (might need to run this in your environment setup, not directly in the app.py file)\n",
        "# !pip install crewai==0.28.8 crewai_tools==0.1.6 langchain_community==0.0.29 sentence-transformers langchain-groq --quiet\n",
        "# !pip install numpy==1.26.4 --quiet\n",
        "# !pip install langchain_huggingface langchain-core==0.1.53 --quiet\n",
        "\n",
        "# Set environment variables\n",
        "# Replace with your actual keys or use Streamlit secrets\n",
        "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY', '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')\n",
        "os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY', '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')\n",
        "\n",
        "\n",
        "def initialize_llm():\n",
        "    \"\"\"Initializes and returns the language model.\"\"\"\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_base=\"https://api.groq.com/openai/v1\",\n",
        "        openai_api_key=os.environ['GROQ_API_KEY'],\n",
        "        model_name=\"llama3-8b-8192\",\n",
        "        temperature=0.1,\n",
        "        max_tokens=1000,\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "def setup_tools():\n",
        "    \"\"\"Sets up and returns the tools used by the agents.\"\"\"\n",
        "    # Download and save the PDF\n",
        "    pdf_url = 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'\n",
        "    try:\n",
        "        response = requests.get(pdf_url)\n",
        "        # Use a temporary file to store the PDF\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=False) as temp_pdf:\n",
        "            temp_pdf.write(response.content)\n",
        "            pdf_path = temp_pdf.name\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error downloading PDF: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    rag_tool = PDFSearchTool(\n",
        "        pdf=pdf_path,\n",
        "        config=dict(\n",
        "            llm=dict(\n",
        "                provider=\"groq\",\n",
        "                config=dict(\n",
        "                    model=\"llama3-8b-8192\",\n",
        "                ),\n",
        "            ),\n",
        "            embedder=dict(\n",
        "                provider=\"huggingface\",\n",
        "                config=dict(\n",
        "                    model=\"BAAI/bge-small-en-v1.5\",\n",
        "                ),\n",
        "            ),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    web_search_tool = TavilySearchResults(k=3)\n",
        "\n",
        "    @tool\n",
        "    def router_tool(question):\n",
        "        \"\"\"Router Function to decide between vectorstore and web search.\"\"\"\n",
        "        if 'self-attention' in question:\n",
        "            return 'vectorstore'\n",
        "        else:\n",
        "            return 'web_search'\n",
        "\n",
        "    return rag_tool, web_search_tool, router_tool, pdf_path\n",
        "\n",
        "def create_agents(llm, rag_tool, web_search_tool, router_tool):\n",
        "    \"\"\"Creates and returns the CrewAI agents.\"\"\"\n",
        "    Router_Agent = Agent(\n",
        "        role='Router',\n",
        "        goal='Route user question to a vectorstore or web search',\n",
        "        backstory=(\n",
        "          \"You are an expert at routing a user question to a vectorstore or web search.\"\n",
        "          \"Use the vectorstore for questions on concepts related to the 'Attention is All You Need' paper.\"\n",
        "          \"Otherwise, use web-search.\"\n",
        "        ),\n",
        "        verbose=True,\n",
        "        allow_delegation=False,\n",
        "        llm=llm,\n",
        "        tools=[router_tool] # Add the router tool to the agent\n",
        "    )\n",
        "\n",
        "    Retriever_Agent = Agent(\n",
        "        role=\"Retriever\",\n",
        "        goal=\"Use the information retrieved from the vectorstore or web search to answer the question\",\n",
        "        backstory=(\n",
        "            \"You are an assistant for question-answering tasks.\"\n",
        "            \"Use the information present in the retrieved context to answer the question.\"\n",
        "            \"You have to provide a clear concise answer.\"\n",
        "        ),\n",
        "        verbose=True,\n",
        "        allow_delegation=False,\n",
        "        llm=llm,\n",
        "        tools=[rag_tool, web_search_tool] # Add both retrieval tools\n",
        "    )\n",
        "\n",
        "    Grader_agent =  Agent(\n",
        "        role='Answer Grader',\n",
        "        goal='Filter out erroneous retrievals',\n",
        "        backstory=(\n",
        "          \"You are a grader assessing relevance of a retrieved document to a user question.\"\n",
        "          \"If the document contains keywords related to the user question, grade it as relevant.\"\n",
        "          \"It does not need to be a stringent test.You have to make sure that the answer is relevant to the question.\"\n",
        "        ),\n",
        "        verbose=True,\n",
        "        allow_delegation=False,\n",
        "        llm=llm,\n",
        "    )\n",
        "\n",
        "    hallucination_grader = Agent(\n",
        "        role=\"Hallucination Grader\",\n",
        "        goal=\"Filter out hallucination\",\n",
        "        backstory=(\n",
        "            \"You are a hallucination grader assessing whether an answer is grounded in / supported by a set of facts.\"\n",
        "            \"Make sure you meticulously review the answer and check if the response provided is in alignment with the question asked\"\n",
        "        ),\n",
        "        verbose=True,\n",
        "        allow_delegation=False,\n",
        "        llm=llm,\n",
        "    )\n",
        "\n",
        "    answer_grader = Agent(\n",
        "        role=\"Answer Grader\",\n",
        "        goal=\"Filter out hallucination from the answer.\",\n",
        "        backstory=(\n",
        "            \"You are a grader assessing whether an answer is useful to resolve a question.\"\n",
        "            \"Make sure you meticulously review the answer and check if it makes sense for the question asked\"\n",
        "            \"If the answer is relevant generate a clear and concise response.\"\n",
        "            \"If the answer generated is not relevant then perform a websearch using 'web_search_tool'\"\n",
        "        ),\n",
        "        verbose=True,\n",
        "        allow_delegation=False,\n",
        "        llm=llm,\n",
        "        tools=[web_search_tool] # Add web search tool for fallback\n",
        "    )\n",
        "\n",
        "    return Router_Agent, Retriever_Agent, Grader_agent, hallucination_grader, answer_grader\n",
        "\n",
        "def create_tasks(Router_Agent, Retriever_Agent, Grader_agent, hallucination_grader, answer_grader):\n",
        "    \"\"\"Creates and returns the CrewAI tasks.\"\"\"\n",
        "    router_task = Task(\n",
        "        description=(\n",
        "            \"Analyse the keywords in the question {question}. \"\n",
        "            \"Based on the keywords decide whether it is eligible for a vectorstore search or a web search. \"\n",
        "            \"Return a single word 'vectorstore' if it is eligible for vectorstore search. \"\n",
        "            \"Return a single word 'websearch' if it is eligible for web search. \"\n",
        "            \"Do not provide any other preamble or explanation.\"\n",
        "        ),\n",
        "        expected_output=(\n",
        "            \"Give a binary choice 'websearch' or 'vectorstore' based on the question. \"\n",
        "            \"Do not provide any other preamble or explanation.\"\n",
        "        ),\n",
        "        agent=Router_Agent,\n",
        "    )\n",
        "\n",
        "    retriever_task = Task(\n",
        "        description=(\n",
        "            \"Based on the response from the router task, extract information for the question {question} with the help of the respective tool. \"\n",
        "            \"If the router task output is 'websearch', use the web_search_tool to retrieve information from the web. \"\n",
        "            \"If the router task output is 'vectorstore', use the rag_tool to retrieve information from the vectorstore.\"\n",
        "        ),\n",
        "        expected_output=(\n",
        "            \"You should analyse the output of the 'router_task'. \"\n",
        "            \"If the response is 'websearch' then use the web_search_tool to retrieve information from the web. \"\n",
        "            \"If the response is 'vectorstore' then use the rag_tool to retrieve information from the vectorstore. \"\n",
        "            \"Return a clear and concise text as response.\"\n",
        "        ),\n",
        "        agent=Retriever_Agent,\n",
        "        context=[router_task],\n",
        "    )\n",
        "\n",
        "    grader_task = Task(\n",
        "        description=(\n",
        "            \"Based on the response from the retriever task for the question {question}, evaluate whether the retrieved content is relevant to the question.\"\n",
        "        ),\n",
        "        expected_output=(\n",
        "            \"Binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \"\n",
        "            \"You must answer 'yes' if the response from the 'retriever_task' is in alignment with the question asked. \"\n",
        "            \"You must answer 'no' if the response from the 'retriever_task' is not in alignment with the question asked. \"\n",
        "            \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"\n",
        "        ),\n",
        "        agent=Grader_agent,\n",
        "        context=[retriever_task],\n",
        "    )\n",
        "\n",
        "    hallucination_task = Task(\n",
        "        description=(\"Based on the response from the grader task for the question {question}, evaluate whether the answer is grounded in / supported by a set of facts.\"),\n",
        "        expected_output=(\n",
        "            \"Binary score 'yes' or 'no' score to indicate whether the answer is in sync with the question asked. \"\n",
        "            \"Respond 'yes' if the answer is useful and contains facts about the question asked. \"\n",
        "            \"Respond 'no' if the answer is not useful and does not contain facts about the question asked. \"\n",
        "            \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"\n",
        "        ),\n",
        "        agent=hallucination_grader,\n",
        "        context=[grader_task],\n",
        "    )\n",
        "\n",
        "    answer_task = Task(\n",
        "        description=(\n",
        "            \"Based on the response from the hallucination task for the question {question}, evaluate whether the answer is useful to resolve the question. \"\n",
        "            \"If the response from the hallucination task is 'yes', return a clear and concise answer based on the previous tasks' context. \"\n",
        "            \"If the response from the hallucination task is 'no', then perform a 'websearch' using the web_search_tool and return the response.\"\n",
        "        ),\n",
        "        expected_output=(\n",
        "            \"Return a clear and concise response if the response from 'hallucination_task' is 'yes'. \"\n",
        "            \"Perform a web search using 'web_search_tool' and return a clear and concise response only if the response from 'hallucination_task' is 'no'. \"\n",
        "            \"Otherwise respond as 'Sorry! unable to find a valid response'.\"\n",
        "        ),\n",
        "        context=[hallucination_task],\n",
        "        agent=answer_grader,\n",
        "    )\n",
        "\n",
        "    return router_task, retriever_task, grader_task, hallucination_task, answer_task\n",
        "\n",
        "def run_crewai_process(question):\n",
        "    \"\"\"Runs the CrewAI process with the given question.\"\"\"\n",
        "    llm = initialize_llm()\n",
        "    rag_tool, web_search_tool, router_tool, pdf_path = setup_tools()\n",
        "\n",
        "    if rag_tool is None or web_search_tool is None or router_tool is None:\n",
        "        return \"Failed to set up tools.\"\n",
        "\n",
        "    Router_Agent, Retriever_Agent, Grader_agent, hallucination_grader, answer_grader = create_agents(llm, rag_tool, web_search_tool, router_tool)\n",
        "    router_task, retriever_task, grader_task, hallucination_task, answer_task = create_tasks(Router_Agent, Retriever_Agent, Grader_agent, hallucination_grader, answer_grader)\n",
        "\n",
        "    rag_crew = Crew(\n",
        "        agents=[Router_Agent, Retriever_Agent, Grader_agent, hallucination_grader, answer_grader],\n",
        "        tasks=[router_task, retriever_task, grader_task, hallucination_task, answer_task],\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    inputs = {\"question\": question}\n",
        "    result = rag_crew.kickoff(inputs=inputs)\n",
        "\n",
        "    # Clean up the temporary PDF file\n",
        "    if os.path.exists(pdf_path):\n",
        "        os.remove(pdf_path)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Streamlit App\n",
        "st.title(\"CrewAI Agentic RAG\")\n",
        "\n",
        "user_question = st.text_input(\"Enter your question:\")\n",
        "\n",
        "if st.button(\"Get Answer\"):\n",
        "    if user_question:\n",
        "        with st.spinner(\"Processing your question...\"):\n",
        "            answer = run_crewai_process(user_question)\n",
        "            st.subheader(\"Answer:\")\n",
        "            st.write(answer)\n",
        "    else:\n",
        "        st.warning(\"Please enter a question.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24c9nJiH-W4i"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSyOJIcQ-yis",
        "outputId": "b0695054-44b4-4f44-dbe2-57549e659e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.68.165.204\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hfTet54IrUl",
        "outputId": "bcf59da3-3fd1-4e44-a7ae-fa5990b0d0cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K\n",
            "changed 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!npm install -g localtunnel@2.0.2 --yes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOIqu6XIJDzR",
        "outputId": "c038e040-177c-4c36-b49b-29be10139817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.68.165.204:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0Kyour url is: https://public-bats-tan.loca.lt\n",
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:918: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `CrewAgentExecutor` to V2.\n",
            "  warn(\n",
            "2025-06-21 09:26:19.943818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750497979.996052    9340 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750497980.008954    9340 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Inserting batches in chromadb: 100% 1/1 [00:15<00:00, 15.81s/it]\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Router\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Analyse the keywords in the question what are LLM's. Based on the keywords decide whether it is eligible for a vectorstore search or a web search. Return a single word 'vectorstore' if it is eligible for vectorstore search. Return a single word 'websearch' if it is eligible for web search. Do not provide any other preamble or explanation.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to analyze the keywords in the question to determine whether it's eligible for a vectorstore search or a web search.\n",
            "\n",
            "Action: router_tool\n",
            "\n",
            "Action Input: {\"question\": \"Analyse the keywords in the question what are LLM's\"}\n",
            "\u001b[0m\u001b[95m \n",
            "\n",
            "web_search\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mFinal Answer: websearch\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Router] Task output: websearch\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Retriever\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the router task, extract information for the question what are LLM's with the help of the respective tool. If the router task output is 'websearch', use the web_search_tool to retrieve information from the web. If the router task output is 'vectorstore', use the rag_tool to retrieve information from the vectorstore.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to extract information about LLM's using the web search tool.\n",
            "\n",
            "Action: web_search_tool\n",
            "Action Input: {\"query\": \"what are LLM's\"}\u001b[0m\u001b[91m \n",
            "\n",
            "Action 'web_search_tool' don't exist, these are the only available Actions: Search a PDF's content: A tool that can be used to semantic search a query the /tmp/tmpu86gystk.pdf PDF's content.\n",
            "tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to extract information about LLM's using the web search tool.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: {\"query\": \"what are LLM's\"}\u001b[0m\u001b[95m \n",
            "\n",
            "[{'url': 'https://www.ibm.com/think/topics/large-language-models', 'content': 'Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. [...] LLMs are a class of [foundation models](https://research.ibm.com/blog/what-are-foundation-models), which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), [...] In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or [code generation tasks](https://www.ibm.com/products/watsonx-code-assistant).'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large [deep learning](https://aws.amazon.com/what-is/deep-learning/) models that are pre-trained on vast amounts of data. The underlying transformer is a set of [neural networks](https://aws.amazon.com/what-is/neural-network/) that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for [generative AI](https://aws.amazon.com/ai/generative-ai/) (artificial intelligence) to produce content based on input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many possible uses. Here are some examples: [...] LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service [chatbots](https://aws.amazon.com/what-is/chatbot/), and simple automated copywriting.\\n\\n### Conversational AI'}, {'url': 'https://www.youtube.com/watch?v=5sLYAQS9sWQ&pp=0gcJCdgAo7VqN5tD', 'content': \"Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\"}, {'url': 'https://ask.library.arizona.edu/faq/407985', 'content': 'A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. [...] A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'In simpler terms, an LLM is a computer program that has been fed enough examples to be able to recognize and interpret human language or other types of complex data. Many LLMs are trained on data that has been gathered from the Internet — thousands or millions of gigabytes\\' worth of text. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM\\'s programmers may use a more curated data set. [...] A large language model (LLM) is a type of [artificial intelligence (AI)](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) program that can recognize and generate text, among other tasks. LLMs are trained on [huge sets of data](https://www.cloudflare.com/learning/ai/big-data/) — hence the name \"large.\" LLMs are built on [machine learning](https://www.cloudflare.com/learning/ai/what-is-machine-learning/): specifically, a type of [neural [...] Large language models (LLMs) are machine learning models that can comprehend and generate human language text. They work by analyzing massive data sets of language.\\n\\n##### Learning Center\\n\\n*   [What is artificial intelligence (AI)?](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/)\\n    \\n*   [What is a large language model (LLM)?](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\\n    \\n*   Machine learning\\n*   Glossary\\n\\n#### Learning Objectives'}]\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "\n",
            "Final Answer: \n",
            "\n",
            "Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or code generation tasks.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Retriever] Task output: Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or code generation tasks.\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the retriever task for the question what are LLM's, evaluate whether the retrieved content is relevant to the question.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "Final Answer: yes\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: yes\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Hallucination Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the grader task for the question what are LLM's, evaluate whether the answer is grounded in / supported by a set of facts.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I will carefully review the answer and assess whether it is grounded in and supported by a set of facts related to the question \"what are LLM's\".\n",
            "\n",
            "Final Answer: yes\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Hallucination Grader] Task output: yes\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the hallucination task for the question what are LLM's, evaluate whether the answer is useful to resolve the question. If the response from the hallucination task is 'yes', return a clear and concise answer based on the previous tasks' context. If the response from the hallucination task is 'no', then perform a 'websearch' using the web_search_tool and return the response.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to evaluate the answer to the question \"what are LLM's\" to determine if it's useful to resolve the question.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: {\"query\": \"what are LLM's\"}\u001b[0m\u001b[95m \n",
            "\n",
            "[{'url': 'https://www.ibm.com/think/topics/large-language-models', 'content': 'Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. [...] LLMs are a class of [foundation models](https://research.ibm.com/blog/what-are-foundation-models), which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), [...] In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or [code generation tasks](https://www.ibm.com/products/watsonx-code-assistant).'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large [deep learning](https://aws.amazon.com/what-is/deep-learning/) models that are pre-trained on vast amounts of data. The underlying transformer is a set of [neural networks](https://aws.amazon.com/what-is/neural-network/) that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for [generative AI](https://aws.amazon.com/ai/generative-ai/) (artificial intelligence) to produce content based on input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many possible uses. Here are some examples: [...] LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service [chatbots](https://aws.amazon.com/what-is/chatbot/), and simple automated copywriting.\\n\\n### Conversational AI'}, {'url': 'https://www.youtube.com/watch?v=5sLYAQS9sWQ&pp=0gcJCdgAo7VqN5tD', 'content': \"Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\"}, {'url': 'https://ask.library.arizona.edu/faq/407985', 'content': 'A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. [...] A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'In simpler terms, an LLM is a computer program that has been fed enough examples to be able to recognize and interpret human language or other types of complex data. Many LLMs are trained on data that has been gathered from the Internet — thousands or millions of gigabytes\\' worth of text. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM\\'s programmers may use a more curated data set. [...] A large language model (LLM) is a type of [artificial intelligence (AI)](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) program that can recognize and generate text, among other tasks. LLMs are trained on [huge sets of data](https://www.cloudflare.com/learning/ai/big-data/) — hence the name \"large.\" LLMs are built on [machine learning](https://www.cloudflare.com/learning/ai/what-is-machine-learning/): specifically, a type of [neural [...] Large language models (LLMs) are machine learning models that can comprehend and generate human language text. They work by analyzing massive data sets of language.\\n\\n##### Learning Center\\n\\n*   [What is artificial intelligence (AI)?](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/)\\n    \\n*   [What is a large language model (LLM)?](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\\n    \\n*   Machine learning\\n*   Glossary\\n\\n#### Learning Objectives'}]\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to evaluate the answer to the question \"what are LLM's\" to determine if it's useful to resolve the question.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: {\"query\": \"what are LLM's\"}\u001b[0m\u001b[95m \n",
            "\n",
            "[{'url': 'https://www.ibm.com/think/topics/large-language-models', 'content': 'Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. [...] LLMs are a class of [foundation models](https://research.ibm.com/blog/what-are-foundation-models), which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), [...] In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or [code generation tasks](https://www.ibm.com/products/watsonx-code-assistant).'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large [deep learning](https://aws.amazon.com/what-is/deep-learning/) models that are pre-trained on vast amounts of data. The underlying transformer is a set of [neural networks](https://aws.amazon.com/what-is/neural-network/) that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for [generative AI](https://aws.amazon.com/ai/generative-ai/) (artificial intelligence) to produce content based on input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many possible uses. Here are some examples: [...] LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service [chatbots](https://aws.amazon.com/what-is/chatbot/), and simple automated copywriting.\\n\\n### Conversational AI'}, {'url': 'https://www.youtube.com/watch?v=5sLYAQS9sWQ&pp=0gcJCdgAo7VqN5tD', 'content': \"Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\"}, {'url': 'https://ask.library.arizona.edu/faq/407985', 'content': 'A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. [...] A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'In simpler terms, an LLM is a computer program that has been fed enough examples to be able to recognize and interpret human language or other types of complex data. Many LLMs are trained on data that has been gathered from the Internet — thousands or millions of gigabytes\\' worth of text. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM\\'s programmers may use a more curated data set. [...] A large language model (LLM) is a type of [artificial intelligence (AI)](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) program that can recognize and generate text, among other tasks. LLMs are trained on [huge sets of data](https://www.cloudflare.com/learning/ai/big-data/) — hence the name \"large.\" LLMs are built on [machine learning](https://www.cloudflare.com/learning/ai/what-is-machine-learning/): specifically, a type of [neural [...] Large language models (LLMs) are machine learning models that can comprehend and generate human language text. They work by analyzing massive data sets of language.\\n\\n##### Learning Center\\n\\n*   [What is artificial intelligence (AI)?](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/)\\n    \\n*   [What is a large language model (LLM)?](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\\n    \\n*   Machine learning\\n*   Glossary\\n\\n#### Learning Objectives'}]\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to re-evaluate the answer to the question \"what are LLM's\" to determine if it's useful to resolve the question.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: {\"query\": \"what are LLM's\"}\u001b[0m\u001b[95m \n",
            "\n",
            "[{'url': 'https://www.ibm.com/think/topics/large-language-models', 'content': 'Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. [...] LLMs are a class of [foundation models](https://research.ibm.com/blog/what-are-foundation-models), which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), [...] In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or [code generation tasks](https://www.ibm.com/products/watsonx-code-assistant).'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large [deep learning](https://aws.amazon.com/what-is/deep-learning/) models that are pre-trained on vast amounts of data. The underlying transformer is a set of [neural networks](https://aws.amazon.com/what-is/neural-network/) that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for [generative AI](https://aws.amazon.com/ai/generative-ai/) (artificial intelligence) to produce content based on input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many possible uses. Here are some examples: [...] LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service [chatbots](https://aws.amazon.com/what-is/chatbot/), and simple automated copywriting.\\n\\n### Conversational AI'}, {'url': 'https://www.youtube.com/watch?v=5sLYAQS9sWQ&pp=0gcJCdgAo7VqN5tD', 'content': \"Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\"}, {'url': 'https://ask.library.arizona.edu/faq/407985', 'content': 'A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. [...] A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'In simpler terms, an LLM is a computer program that has been fed enough examples to be able to recognize and interpret human language or other types of complex data. Many LLMs are trained on data that has been gathered from the Internet — thousands or millions of gigabytes\\' worth of text. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM\\'s programmers may use a more curated data set. [...] A large language model (LLM) is a type of [artificial intelligence (AI)](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) program that can recognize and generate text, among other tasks. LLMs are trained on [huge sets of data](https://www.cloudflare.com/learning/ai/big-data/) — hence the name \"large.\" LLMs are built on [machine learning](https://www.cloudflare.com/learning/ai/what-is-machine-learning/): specifically, a type of [neural [...] Large language models (LLMs) are machine learning models that can comprehend and generate human language text. They work by analyzing massive data sets of language.\\n\\n##### Learning Center\\n\\n*   [What is artificial intelligence (AI)?](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/)\\n    \\n*   [What is a large language model (LLM)?](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\\n    \\n*   Machine learning\\n*   Glossary\\n\\n#### Learning Objectives'}]\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to evaluate the answer to the question \"what are LLM's\" to determine if it's useful to resolve the question.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: {\"query\": \"what are LLM's\"}\u001b[0m\u001b[95m \n",
            "\n",
            "[{'url': 'https://www.ibm.com/think/topics/large-language-models', 'content': 'Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. [...] LLMs are a class of [foundation models](https://research.ibm.com/blog/what-are-foundation-models), which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), [...] In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or [code generation tasks](https://www.ibm.com/products/watsonx-code-assistant).'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large [deep learning](https://aws.amazon.com/what-is/deep-learning/) models that are pre-trained on vast amounts of data. The underlying transformer is a set of [neural networks](https://aws.amazon.com/what-is/neural-network/) that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for [generative AI](https://aws.amazon.com/ai/generative-ai/) (artificial intelligence) to produce content based on input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many possible uses. Here are some examples: [...] LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service [chatbots](https://aws.amazon.com/what-is/chatbot/), and simple automated copywriting.\\n\\n### Conversational AI'}, {'url': 'https://www.youtube.com/watch?v=5sLYAQS9sWQ&pp=0gcJCdgAo7VqN5tD', 'content': \"Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\"}, {'url': 'https://ask.library.arizona.edu/faq/407985', 'content': 'A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. [...] A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'In simpler terms, an LLM is a computer program that has been fed enough examples to be able to recognize and interpret human language or other types of complex data. Many LLMs are trained on data that has been gathered from the Internet — thousands or millions of gigabytes\\' worth of text. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM\\'s programmers may use a more curated data set. [...] A large language model (LLM) is a type of [artificial intelligence (AI)](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) program that can recognize and generate text, among other tasks. LLMs are trained on [huge sets of data](https://www.cloudflare.com/learning/ai/big-data/) — hence the name \"large.\" LLMs are built on [machine learning](https://www.cloudflare.com/learning/ai/what-is-machine-learning/): specifically, a type of [neural [...] Large language models (LLMs) are machine learning models that can comprehend and generate human language text. They work by analyzing massive data sets of language.\\n\\n##### Learning Center\\n\\n*   [What is artificial intelligence (AI)?](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/)\\n    \\n*   [What is a large language model (LLM)?](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\\n    \\n*   Machine learning\\n*   Glossary\\n\\n#### Learning Objectives'}]\n",
            "\u001b[00m\n",
            "\u001b[31m──\u001b[0m\u001b[31m────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────────\u001b[0m\u001b[31m──\u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mexec_code.py\u001b[0m: \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m128\u001b[0m in \u001b[92mexec_func_with_error_handling\u001b[0m                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mscript_runner\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m669\u001b[0m in \u001b[92mcode_to_exec\u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m263\u001b[0m in \u001b[92m<module>\u001b[0m                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m260 \u001b[0m\u001b[94mif\u001b[0m st.button(\u001b[33m\"\u001b[0m\u001b[33mGet Answer\u001b[0m\u001b[33m\"\u001b[0m):                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m user_question:                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m262 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m st.spinner(\u001b[33m\"\u001b[0m\u001b[33mProcessing your question...\u001b[0m\u001b[33m\"\u001b[0m):                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m263 \u001b[2m│   │   │   \u001b[0manswer = \u001b[1;4mrun_crewai_process(user_question)\u001b[0m                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m264 \u001b[0m\u001b[2m│   │   │   \u001b[0mst.subheader(\u001b[33m\"\u001b[0m\u001b[33mAnswer:\u001b[0m\u001b[33m\"\u001b[0m)                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m265 \u001b[0m\u001b[2m│   │   │   \u001b[0mst.write(answer)                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m266 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m247\u001b[0m in \u001b[92mrun_crewai_process\u001b[0m                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m244 \u001b[0m\u001b[2m│   \u001b[0m)                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m245 \u001b[0m\u001b[2m│   \u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m246 \u001b[0m\u001b[2m│   \u001b[0minputs = {\u001b[33m\"\u001b[0m\u001b[33mquestion\u001b[0m\u001b[33m\"\u001b[0m: question}                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m247 \u001b[2m│   \u001b[0mresult = \u001b[1;4mrag_crew.kickoff(inputs=inputs)\u001b[0m                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m248 \u001b[0m\u001b[2m│   \u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m249 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Clean up the temporary PDF file\u001b[0m                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m250 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m os.path.exists(pdf_path):                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/\u001b[0m\u001b[1;33mcrew.py\u001b[0m:\u001b[94m252\u001b[0m in \u001b[92mkickoff\u001b[0m                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m249 \u001b[0m\u001b[2m│   │   \u001b[0mmetrics = []                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m250 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m251 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.process == Process.sequential:                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m252 \u001b[2m│   │   │   \u001b[0mresult = \u001b[1;4;96mself\u001b[0m\u001b[1;4m._run_sequential_process()\u001b[0m                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m253 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.process == Process.hierarchical:                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m254 \u001b[0m\u001b[2m│   │   │   \u001b[0mresult, manager_metrics = \u001b[96mself\u001b[0m._run_hierarchical_process()         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m255 \u001b[0m\u001b[2m│   │   │   \u001b[0mmetrics.append(manager_metrics)                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/\u001b[0m\u001b[1;33mcrew.py\u001b[0m:\u001b[94m293\u001b[0m in                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_run_sequential_process\u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0magent=role, task=task.description, status=\u001b[33m\"\u001b[0m\u001b[33mstarted\u001b[0m\u001b[33m\"\u001b[0m        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m292 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m293 \u001b[2m│   │   │   \u001b[0moutput = \u001b[1;4mtask.execute(context=task_output)\u001b[0m                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m294 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m task.async_execution:                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m295 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtask_output = output                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m296 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/\u001b[0m\u001b[1;33mtask.py\u001b[0m:\u001b[94m173\u001b[0m in \u001b[92mexecute\u001b[0m                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.thread.start()                                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m173 \u001b[2m│   │   │   \u001b[0mresult = \u001b[96mself\u001b[0m._execute(                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtask=\u001b[96mself\u001b[0m,                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   │   │   │   \u001b[0magent=agent,                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m176 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcontext=context,                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/\u001b[0m\u001b[1;33mtask.py\u001b[0m:\u001b[94m182\u001b[0m in \u001b[92m_execute\u001b[0m               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m179 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m result                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m180 \u001b[0m\u001b[2m│   \u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m181 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_execute\u001b[0m(\u001b[96mself\u001b[0m, agent, task, context, tools):                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m182 \u001b[2m│   │   \u001b[0mresult = \u001b[1;4magent.execute_task(\u001b[0m                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m183 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mtask=task,\u001b[0m                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m184 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mcontext=context,\u001b[0m                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m185 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mtools=tools,\u001b[0m                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/\u001b[0m\u001b[1;33magent.py\u001b[0m:\u001b[94m221\u001b[0m in \u001b[92mexecute_task\u001b[0m          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m218 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.agent_executor.tools_description = render_text_description(parsed \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m219 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.agent_executor.tools_names = \u001b[96mself\u001b[0m.__tools_names(parsed_tools)     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m221 \u001b[2m│   │   \u001b[0mresult = \u001b[96mself\u001b[0m.agent_executor.invoke(                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m│   │   │   \u001b[0m{                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m223 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33minput\u001b[0m\u001b[33m\"\u001b[0m: task_prompt,                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m224 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mtool_names\u001b[0m\u001b[33m\"\u001b[0m: \u001b[96mself\u001b[0m.agent_executor.tools_names,                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain/chains/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m163\u001b[0m in \u001b[92minvoke\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m160 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m161 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mBaseException\u001b[0m \u001b[94mas\u001b[0m e:                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0mrun_manager.on_chain_error(e)                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m163 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m e\u001b[0m                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0mrun_manager.on_chain_end(outputs)                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m165 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m include_run_info:                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain/chains/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m153\u001b[0m in \u001b[92minvoke\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m150 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._validate_inputs(inputs)                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   │   \u001b[0moutputs = (                                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m153 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._call(inputs, run_manager=run_manager)                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m154 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m new_arg_supported                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m155 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m \u001b[96mself\u001b[0m._call(inputs)                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/agents/\u001b[0m\u001b[1;33mexecutor.py\u001b[0m:\u001b[94m124\u001b[0m in \u001b[92m_call\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# We now enter the agent loop (until it returns something).\u001b[0m            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[96mself\u001b[0m._should_continue(\u001b[96mself\u001b[0m.iterations, time_elapsed):            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m123 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m.request_within_rpm_limit \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.request_within_rpm_li \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m124 \u001b[2m│   │   │   │   \u001b[0mnext_step_output = \u001b[96mself\u001b[0m._take_next_step(                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mname_to_tool_map,                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcolor_mapping,                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m127 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0minputs,                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain/agents/\u001b[0m\u001b[1;33magent.py\u001b[0m:\u001b[94m1138\u001b[0m in            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_take_next_step\u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1135 \u001b[0m\u001b[2m│   │   \u001b[0mrun_manager: Optional[CallbackManagerForChainRun] = \u001b[94mNone\u001b[0m,             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1136 \u001b[0m\u001b[2m│   \u001b[0m) -> Union[AgentFinish, List[Tuple[AgentAction, \u001b[96mstr\u001b[0m]]]:                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1137 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._consume_next_step(                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1138 \u001b[2m│   │   │   \u001b[0m[                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1139 \u001b[0m\u001b[2m│   │   │   │   \u001b[0ma                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1140 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m a \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._iter_next_step(                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1141 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mname_to_tool_map,                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain/agents/\u001b[0m\u001b[1;33magent.py\u001b[0m:\u001b[94m1138\u001b[0m in \u001b[92m<listcomp>\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1135 \u001b[0m\u001b[2m│   │   \u001b[0mrun_manager: Optional[CallbackManagerForChainRun] = \u001b[94mNone\u001b[0m,             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1136 \u001b[0m\u001b[2m│   \u001b[0m) -> Union[AgentFinish, List[Tuple[AgentAction, \u001b[96mstr\u001b[0m]]]:                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1137 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._consume_next_step(                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1138 \u001b[2m│   │   │   \u001b[0m[                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1139 \u001b[0m\u001b[2m│   │   │   │   \u001b[0ma                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1140 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m a \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._iter_next_step(                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1141 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mname_to_tool_map,                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/agents/\u001b[0m\u001b[1;33mexecutor.py\u001b[0m:\u001b[94m186\u001b[0m in             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_iter_next_step\u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m│   │   │   \u001b[0mintermediate_steps = \u001b[96mself\u001b[0m._prepare_intermediate_steps(intermediate \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m184 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m185 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Call the LLM to see what to do.\u001b[0m                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m186 \u001b[2m│   │   │   \u001b[0moutput = \u001b[96mself\u001b[0m.agent.plan(                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m187 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mintermediate_steps,                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcallbacks=run_manager.get_child() \u001b[94mif\u001b[0m run_manager \u001b[94melse\u001b[0m \u001b[94mNone\u001b[0m,    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**inputs,                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain/agents/\u001b[0m\u001b[1;33magent.py\u001b[0m:\u001b[94m397\u001b[0m in \u001b[92mplan\u001b[0m        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 394 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# when using stream_log with the Agent Executor.\u001b[0m                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 395 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Because the response from the plan is not a generator, we need \u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 396 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# accumulate the output into final output and return that.\u001b[0m        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 397 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mfor\u001b[0m\u001b[1;4m chunk \u001b[0m\u001b[1;4;95min\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.runnable.stream(inputs, config={\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mcallbacks\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m: ca\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 398 \u001b[0m\u001b[1;2;4m│   │   │   │   \u001b[0m\u001b[1;4;94mif\u001b[0m\u001b[1;4m final_output \u001b[0m\u001b[1;4;95mis\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;94mNone\u001b[0m\u001b[1;4m:\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 399 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4mfinal_output = chunk\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 400 \u001b[0m\u001b[1;2;4m│   │   │   │   \u001b[0m\u001b[1;4;94melse\u001b[0m\u001b[1;4m:\u001b[0m                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m2875\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mstream\u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2872 \u001b[0m\u001b[2m│   │   \u001b[0mconfig: Optional[RunnableConfig] = \u001b[94mNone\u001b[0m,                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2873 \u001b[0m\u001b[2m│   │   \u001b[0m**kwargs: Optional[Any],                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2874 \u001b[0m\u001b[2m│   \u001b[0m) -> Iterator[Output]:                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m2875 \u001b[2m│   │   \u001b[0m\u001b[1;4;94myield from\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.transform(\u001b[0m\u001b[1;4;96miter\u001b[0m\u001b[1;4m([\u001b[0m\u001b[1;4;96minput\u001b[0m\u001b[1;4m]), config, **kwargs)\u001b[0m            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2876 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2877 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92matransform\u001b[0m(                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2878 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m2862\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mtransform\u001b[0m                                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2859 \u001b[0m\u001b[2m│   │   \u001b[0mconfig: Optional[RunnableConfig] = \u001b[94mNone\u001b[0m,                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2860 \u001b[0m\u001b[2m│   │   \u001b[0m**kwargs: Optional[Any],                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2861 \u001b[0m\u001b[2m│   \u001b[0m) -> Iterator[Output]:                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m2862 \u001b[2m│   │   \u001b[0m\u001b[94myield from\u001b[0m \u001b[96mself\u001b[0m._transform_stream_with_config(                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2863 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96minput\u001b[0m,                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2864 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._transform,                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2865 \u001b[0m\u001b[2m│   │   │   \u001b[0mpatch_config(config, run_name=(config \u001b[95mor\u001b[0m {}).get(\u001b[33m\"\u001b[0m\u001b[33mrun_name\u001b[0m\u001b[33m\"\u001b[0m) \u001b[95mor\u001b[0m \u001b[96ms\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m1881\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_transform_stream_with_config\u001b[0m                                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1878 \u001b[0m\u001b[2m│   │   │   \u001b[0miterator = context.run(transformer, input_for_transform, **kwargs \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1879 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1880 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1881 \u001b[2m│   │   │   │   │   \u001b[0mchunk: Output = context.run(\u001b[96mnext\u001b[0m, iterator)  \u001b[2m# type: igno\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1882 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94myield\u001b[0m chunk                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1883 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m final_output_supported:                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1884 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m final_output \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m2826\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_transform\u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2823 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m),                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2824 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2825 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m2826 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mfor\u001b[0m\u001b[1;4m output \u001b[0m\u001b[1;4;95min\u001b[0m\u001b[1;4m final_pipeline:\u001b[0m                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2827 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;94myield\u001b[0m\u001b[1;4m output\u001b[0m                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2828 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2829 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_atransform\u001b[0m(                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m1282\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mtransform\u001b[0m                                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1279 \u001b[0m\u001b[2m│   │   \u001b[0mfinal: Input                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1280 \u001b[0m\u001b[2m│   │   \u001b[0mgot_first_val = \u001b[94mFalse\u001b[0m                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1281 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1282 \u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m ichunk \u001b[95min\u001b[0m \u001b[96minput\u001b[0m:                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1283 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# The default implementation of transform is to buffer input and\u001b[0m  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1284 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# then call stream.\u001b[0m                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1285 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# It'll attempt to gather all input into a single chunk using\u001b[0m     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m4736\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mtransform\u001b[0m                                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4733 \u001b[0m\u001b[2m│   │   \u001b[0mconfig: Optional[RunnableConfig] = \u001b[94mNone\u001b[0m,                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4734 \u001b[0m\u001b[2m│   │   \u001b[0m**kwargs: Any,                                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4735 \u001b[0m\u001b[2m│   \u001b[0m) -> Iterator[Output]:                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m4736 \u001b[2m│   │   \u001b[0m\u001b[1;4;94myield from\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.bound.transform(\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4737 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;96minput\u001b[0m\u001b[1;4m,\u001b[0m                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4738 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m._merge_configs(config),\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4739 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4m**{**\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.kwargs, **kwargs},\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m1300\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mtransform\u001b[0m                                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1297 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mfinal = ichunk                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1298 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1299 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m got_first_val:                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1300 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94myield from\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.stream(final, config, **kwargs)\u001b[0m                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1301 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1302 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92matransform\u001b[0m(                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1303 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/\u001b[0m\u001b[1;33mchat_models.p\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m249\u001b[0m in \u001b[92mstream\u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m246 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mgenerations=[[generation]] \u001b[94mif\u001b[0m generation \u001b[94melse\u001b[0m []       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m247 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m),                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m248 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m249 \u001b[2m│   │   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m e\u001b[0m                                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m250 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m251 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mrun_manager.on_llm_end(LLMResult(generations=[[generation]]))  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m252 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/\u001b[0m\u001b[1;33mchat_models.p\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m229\u001b[0m in \u001b[92mstream\u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m227 \u001b[0m\u001b[2m│   │   │   \u001b[0mgeneration: Optional[ChatGenerationChunk] = \u001b[94mNone\u001b[0m                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m228 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m229 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m chunk \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._stream(messages, stop=stop, **kwargs):      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m230 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m chunk.message.id \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m231 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mchunk.message.id = \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mrun-\u001b[0m\u001b[33m{\u001b[0mrun_manager.run_id\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m232 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mchunk.message.response_metadata = _gen_info_and_msg_metada \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m480\u001b[0m in  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_stream\u001b[0m                                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 477 \u001b[0m\u001b[2m│   │   \u001b[0mparams = {**params, **kwargs, \u001b[33m\"\u001b[0m\u001b[33mstream\u001b[0m\u001b[33m\"\u001b[0m: \u001b[94mTrue\u001b[0m}                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 478 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 479 \u001b[0m\u001b[2m│   │   \u001b[0mdefault_chunk_class = AIMessageChunk                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 480 \u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m.client.create(messages=message_dicts, **params)\u001b[0m \u001b[94mas\u001b[0m response \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 481 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m chunk \u001b[95min\u001b[0m response:                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 482 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(chunk, \u001b[96mdict\u001b[0m):                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 483 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mchunk = chunk.model_dump()                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/openai/_utils/\u001b[0m\u001b[1;33m_utils.py\u001b[0m:\u001b[94m287\u001b[0m in \u001b[92mwrapper\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m284 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m285 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mmsg = \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mMissing required argument: \u001b[0m\u001b[33m{\u001b[0mquote(missing[\u001b[94m0\u001b[0m])\u001b[33m}\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m286 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mTypeError\u001b[0m(msg)                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m287 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mfunc(*args, **kwargs)\u001b[0m                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m288 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m289 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m wrapper  \u001b[2m# type: ignore\u001b[0m                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m290 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/\u001b[0m\u001b[1;33mcompletion\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33ms.py\u001b[0m:\u001b[94m925\u001b[0m in \u001b[92mcreate\u001b[0m                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 922 \u001b[0m\u001b[2m│   │   \u001b[0mtimeout: \u001b[96mfloat\u001b[0m | httpx.Timeout | \u001b[94mNone\u001b[0m | NotGiven = NOT_GIVEN,         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 923 \u001b[0m\u001b[2m│   \u001b[0m) -> ChatCompletion | Stream[ChatCompletionChunk]:                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 924 \u001b[0m\u001b[2m│   │   \u001b[0mvalidate_response_format(response_format)                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 925 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._post(                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 926 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m/chat/completions\u001b[0m\u001b[33m\"\u001b[0m,                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 927 \u001b[0m\u001b[2m│   │   │   \u001b[0mbody=maybe_transform(                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 928 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m{                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/openai/\u001b[0m\u001b[1;33m_base_client.py\u001b[0m:\u001b[94m1242\u001b[0m in \u001b[92mpost\u001b[0m          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1239 \u001b[0m\u001b[2m│   │   \u001b[0mopts = FinalRequestOptions.construct(                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1240 \u001b[0m\u001b[2m│   │   │   \u001b[0mmethod=\u001b[33m\"\u001b[0m\u001b[33mpost\u001b[0m\u001b[33m\"\u001b[0m, url=path, json_data=body, files=to_httpx_files(fil \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1241 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1242 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m cast(ResponseT, \u001b[1;4;96mself\u001b[0m\u001b[1;4m.request(cast_to, opts, stream=stream, str\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1243 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1244 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mpatch\u001b[0m(                                                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1245 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/openai/\u001b[0m\u001b[1;33m_base_client.py\u001b[0m:\u001b[94m1037\u001b[0m in \u001b[92mrequest\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1034 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0merr.response.read()                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1035 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1036 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlog.debug(\u001b[33m\"\u001b[0m\u001b[33mRe-raising status error\u001b[0m\u001b[33m\"\u001b[0m)                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1037 \u001b[2m│   │   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m._make_status_error_from_response(err.response) \u001b[0m\u001b[1;4;94mfro\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1038 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1039 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1040 \u001b[0m                                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[1;91mAPIStatusError: \u001b[0mError code: \u001b[1;36m413\u001b[0m - \u001b[1m{\u001b[0m\u001b[32m'error'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'message'\u001b[0m: \u001b[32m'Request too large for model \u001b[0m\n",
            "\u001b[32m`llama3-8b-8192` in organization `org_01jv986wb6e4esj2pz6rqtmahc` service tier \u001b[0m\n",
            "\u001b[32m`on_demand` on tokens per minute \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTPM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: Limit 6000, Requested 6768, please reduce your \u001b[0m\n",
            "\u001b[32mmessage size and try again. Need more tokens? Upgrade to Dev Tier today at \u001b[0m\n",
            "\u001b[32mhttps://console.groq.com/settings/billing'\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'tokens'\u001b[0m, \u001b[32m'code'\u001b[0m: \n",
            "\u001b[32m'rate_limit_exceeded'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n",
            "Inserting batches in chromadb: 100% 1/1 [00:16<00:00, 16.73s/it]\n",
            "2025-06-21 09:45:14,905 - 136533803734592 - __init__.py-__init__:537 - WARNING: Overriding of current TracerProvider is not allowed\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Router\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Analyse the keywords in the question what are LLM's. Based on the keywords decide whether it is eligible for a vectorstore search or a web search. Return a single word 'vectorstore' if it is eligible for vectorstore search. Return a single word 'websearch' if it is eligible for web search. Do not provide any other preamble or explanation.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to analyze the keywords in the question to determine whether it's eligible for a vectorstore search or a web search.\n",
            "\n",
            "Action: router_tool\n",
            "\n",
            "Action Input: {\"question\": \"Analyse the keywords in the question what are LLM's\"}\n",
            "\u001b[0m\u001b[95m \n",
            "\n",
            "web_search\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mFinal Answer: websearch\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Router] Task output: websearch\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Retriever\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the router task, extract information for the question what are LLM's with the help of the respective tool. If the router task output is 'websearch', use the web_search_tool to retrieve information from the web. If the router task output is 'vectorstore', use the rag_tool to retrieve information from the vectorstore.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to extract information about LLM's using the web search tool.\n",
            "\n",
            "Action: web_search_tool\n",
            "\n",
            "Action Input: {\"query\": \"what are LLM's\"}\n",
            "\u001b[0m\u001b[91m \n",
            "\n",
            "Action 'web_search_tool' don't exist, these are the only available Actions: Search a PDF's content: A tool that can be used to semantic search a query the /tmp/tmps3vqz5q_.pdf PDF's content.\n",
            "tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to extract information about LLM's using the web search tool.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "\n",
            "Action Input: {\"query\": \"what are LLM's\"}\n",
            "\u001b[0m\u001b[95m \n",
            "\n",
            "[{'url': 'https://www.ibm.com/think/topics/large-language-models', 'content': 'Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. [...] LLMs are a class of [foundation models](https://research.ibm.com/blog/what-are-foundation-models), which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), [...] In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or [code generation tasks](https://www.ibm.com/products/watsonx-code-assistant).'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large [deep learning](https://aws.amazon.com/what-is/deep-learning/) models that are pre-trained on vast amounts of data. The underlying transformer is a set of [neural networks](https://aws.amazon.com/what-is/neural-network/) that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for [generative AI](https://aws.amazon.com/ai/generative-ai/) (artificial intelligence) to produce content based on input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many possible uses. Here are some examples: [...] LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service [chatbots](https://aws.amazon.com/what-is/chatbot/), and simple automated copywriting.\\n\\n### Conversational AI'}, {'url': 'https://www.youtube.com/watch?v=5sLYAQS9sWQ&pp=0gcJCdgAo7VqN5tD', 'content': \"Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\"}, {'url': 'https://ask.library.arizona.edu/faq/407985', 'content': 'A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. [...] A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'In simpler terms, an LLM is a computer program that has been fed enough examples to be able to recognize and interpret human language or other types of complex data. Many LLMs are trained on data that has been gathered from the Internet — thousands or millions of gigabytes\\' worth of text. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM\\'s programmers may use a more curated data set. [...] A large language model (LLM) is a type of [artificial intelligence (AI)](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) program that can recognize and generate text, among other tasks. LLMs are trained on [huge sets of data](https://www.cloudflare.com/learning/ai/big-data/) — hence the name \"large.\" LLMs are built on [machine learning](https://www.cloudflare.com/learning/ai/what-is-machine-learning/): specifically, a type of [neural [...] Large language models (LLMs) are machine learning models that can comprehend and generate human language text. They work by analyzing massive data sets of language.\\n\\n##### Learning Center\\n\\n*   [What is artificial intelligence (AI)?](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/)\\n    \\n*   [What is a large language model (LLM)?](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\\n    \\n*   Machine learning\\n*   Glossary\\n\\n#### Learning Objectives'}]\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "\n",
            "Final Answer:\n",
            "\n",
            "Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. They are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or code generation tasks.\n",
            "\n",
            "They are a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.\n",
            "\n",
            "They are built on machine learning: specifically, a type of neural network that can comprehend and generate human language text. They work by analyzing massive data sets of language.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Retriever] Task output: Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. They are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or code generation tasks.\n",
            "\n",
            "They are a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.\n",
            "\n",
            "They are built on machine learning: specifically, a type of neural network that can comprehend and generate human language text. They work by analyzing massive data sets of language.\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the retriever task for the question what are LLM's, evaluate whether the retrieved content is relevant to the question.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "\n",
            "Final Answer: yes\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: yes\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Hallucination Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the grader task for the question what are LLM's, evaluate whether the answer is grounded in / supported by a set of facts.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I will carefully review the answer and assess whether it is grounded in and supported by a set of facts related to the question \"what are LLM's\".\n",
            "\n",
            "Final Answer: yes\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Hallucination Grader] Task output: yes\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the hallucination task for the question what are LLM's, evaluate whether the answer is useful to resolve the question. If the response from the hallucination task is 'yes', return a clear and concise answer based on the previous tasks' context. If the response from the hallucination task is 'no', then perform a 'websearch' using the web_search_tool and return the response.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to evaluate the answer to the question \"what are LLM's\" to determine if it's useful to resolve the question.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: {\"query\": \"what are LLM's\"}\u001b[0m\u001b[95m \n",
            "\n",
            "[{'url': 'https://www.ibm.com/think/topics/large-language-models', 'content': 'Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. [...] LLMs are a class of [foundation models](https://research.ibm.com/blog/what-are-foundation-models), which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), [...] In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or [code generation tasks](https://www.ibm.com/products/watsonx-code-assistant).'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large [deep learning](https://aws.amazon.com/what-is/deep-learning/) models that are pre-trained on vast amounts of data. The underlying transformer is a set of [neural networks](https://aws.amazon.com/what-is/neural-network/) that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for [generative AI](https://aws.amazon.com/ai/generative-ai/) (artificial intelligence) to produce content based on input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many possible uses. Here are some examples: [...] LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service [chatbots](https://aws.amazon.com/what-is/chatbot/), and simple automated copywriting.\\n\\n### Conversational AI'}, {'url': 'https://www.youtube.com/watch?v=5sLYAQS9sWQ&pp=0gcJCdgAo7VqN5tD', 'content': \"Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\"}, {'url': 'https://ask.library.arizona.edu/faq/407985', 'content': 'A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. [...] A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'In simpler terms, an LLM is a computer program that has been fed enough examples to be able to recognize and interpret human language or other types of complex data. Many LLMs are trained on data that has been gathered from the Internet — thousands or millions of gigabytes\\' worth of text. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM\\'s programmers may use a more curated data set. [...] A large language model (LLM) is a type of [artificial intelligence (AI)](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) program that can recognize and generate text, among other tasks. LLMs are trained on [huge sets of data](https://www.cloudflare.com/learning/ai/big-data/) — hence the name \"large.\" LLMs are built on [machine learning](https://www.cloudflare.com/learning/ai/what-is-machine-learning/): specifically, a type of [neural [...] Large language models (LLMs) are machine learning models that can comprehend and generate human language text. They work by analyzing massive data sets of language.\\n\\n##### Learning Center\\n\\n*   [What is artificial intelligence (AI)?](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/)\\n    \\n*   [What is a large language model (LLM)?](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\\n    \\n*   Machine learning\\n*   Glossary\\n\\n#### Learning Objectives'}]\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to evaluate the answer to the question \"what are LLM's\" to determine if it's useful to resolve the question.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: {\"query\": \"what are LLM's\"}\u001b[0m\u001b[95m \n",
            "\n",
            "[{'url': 'https://www.ibm.com/think/topics/large-language-models', 'content': 'Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. [...] LLMs are a class of [foundation models](https://research.ibm.com/blog/what-are-foundation-models), which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), [...] In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or [code generation tasks](https://www.ibm.com/products/watsonx-code-assistant).'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large [deep learning](https://aws.amazon.com/what-is/deep-learning/) models that are pre-trained on vast amounts of data. The underlying transformer is a set of [neural networks](https://aws.amazon.com/what-is/neural-network/) that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for [generative AI](https://aws.amazon.com/ai/generative-ai/) (artificial intelligence) to produce content based on input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many possible uses. Here are some examples: [...] LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service [chatbots](https://aws.amazon.com/what-is/chatbot/), and simple automated copywriting.\\n\\n### Conversational AI'}, {'url': 'https://www.youtube.com/watch?v=5sLYAQS9sWQ&pp=0gcJCdgAo7VqN5tD', 'content': \"Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\"}, {'url': 'https://ask.library.arizona.edu/faq/407985', 'content': 'A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. [...] A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'In simpler terms, an LLM is a computer program that has been fed enough examples to be able to recognize and interpret human language or other types of complex data. Many LLMs are trained on data that has been gathered from the Internet — thousands or millions of gigabytes\\' worth of text. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM\\'s programmers may use a more curated data set. [...] A large language model (LLM) is a type of [artificial intelligence (AI)](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) program that can recognize and generate text, among other tasks. LLMs are trained on [huge sets of data](https://www.cloudflare.com/learning/ai/big-data/) — hence the name \"large.\" LLMs are built on [machine learning](https://www.cloudflare.com/learning/ai/what-is-machine-learning/): specifically, a type of [neural [...] Large language models (LLMs) are machine learning models that can comprehend and generate human language text. They work by analyzing massive data sets of language.\\n\\n##### Learning Center\\n\\n*   [What is artificial intelligence (AI)?](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/)\\n    \\n*   [What is a large language model (LLM)?](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\\n    \\n*   Machine learning\\n*   Glossary\\n\\n#### Learning Objectives'}]\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to re-evaluate the answer to the question \"what are LLM's\" to determine if it's useful to resolve the question.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: {\"query\": \"what are LLM's\"}\u001b[0m\u001b[95m \n",
            "\n",
            "[{'url': 'https://www.ibm.com/think/topics/large-language-models', 'content': 'Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. [...] LLMs are a class of [foundation models](https://research.ibm.com/blog/what-are-foundation-models), which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), [...] In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or [code generation tasks](https://www.ibm.com/products/watsonx-code-assistant).'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large [deep learning](https://aws.amazon.com/what-is/deep-learning/) models that are pre-trained on vast amounts of data. The underlying transformer is a set of [neural networks](https://aws.amazon.com/what-is/neural-network/) that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for [generative AI](https://aws.amazon.com/ai/generative-ai/) (artificial intelligence) to produce content based on input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many possible uses. Here are some examples: [...] LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service [chatbots](https://aws.amazon.com/what-is/chatbot/), and simple automated copywriting.\\n\\n### Conversational AI'}, {'url': 'https://www.youtube.com/watch?v=5sLYAQS9sWQ&pp=0gcJCdgAo7VqN5tD', 'content': \"Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\"}, {'url': 'https://ask.library.arizona.edu/faq/407985', 'content': 'A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. [...] A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'In simpler terms, an LLM is a computer program that has been fed enough examples to be able to recognize and interpret human language or other types of complex data. Many LLMs are trained on data that has been gathered from the Internet — thousands or millions of gigabytes\\' worth of text. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM\\'s programmers may use a more curated data set. [...] A large language model (LLM) is a type of [artificial intelligence (AI)](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) program that can recognize and generate text, among other tasks. LLMs are trained on [huge sets of data](https://www.cloudflare.com/learning/ai/big-data/) — hence the name \"large.\" LLMs are built on [machine learning](https://www.cloudflare.com/learning/ai/what-is-machine-learning/): specifically, a type of [neural [...] Large language models (LLMs) are machine learning models that can comprehend and generate human language text. They work by analyzing massive data sets of language.\\n\\n##### Learning Center\\n\\n*   [What is artificial intelligence (AI)?](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/)\\n    \\n*   [What is a large language model (LLM)?](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\\n    \\n*   Machine learning\\n*   Glossary\\n\\n#### Learning Objectives'}]\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to evaluate the answer to the question \"what are LLM's\" to determine if it's useful to resolve the question.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: {\"query\": \"what are LLM's\"}\u001b[0m\u001b[95m \n",
            "\n",
            "[{'url': 'https://www.ibm.com/think/topics/large-language-models', 'content': 'Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks. [...] LLMs are a class of [foundation models](https://research.ibm.com/blog/what-are-foundation-models), which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications, as well as resolve a multitude of tasks. This is in stark contrast to the idea of building and training domain specific models for each of these use cases individually, which is prohibitive under many criteria (most importantly cost and infrastructure), [...] In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or [code generation tasks](https://www.ibm.com/products/watsonx-code-assistant).'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large [deep learning](https://aws.amazon.com/what-is/deep-learning/) models that are pre-trained on vast amounts of data. The underlying transformer is a set of [neural networks](https://aws.amazon.com/what-is/neural-network/) that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for [generative AI](https://aws.amazon.com/ai/generative-ai/) (artificial intelligence) to produce content based on input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many possible uses. Here are some examples: [...] LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service [chatbots](https://aws.amazon.com/what-is/chatbot/), and simple automated copywriting.\\n\\n### Conversational AI'}, {'url': 'https://www.youtube.com/watch?v=5sLYAQS9sWQ&pp=0gcJCdgAo7VqN5tD', 'content': \"Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\"}, {'url': 'https://ask.library.arizona.edu/faq/407985', 'content': 'A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content. [...] A large language model (LLM) is a type of artificial intelligence that can generate human language and perform related tasks. These models are trained on huge datasets, often containing billions of words. By analyzing all this data, the LLM learns patterns and rules of language, similar to how a human learns to communicate through exposure to language. LLMs can perform various language tasks, such as answering questions, summarizing text, translating between languages, and writing content.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'In simpler terms, an LLM is a computer program that has been fed enough examples to be able to recognize and interpret human language or other types of complex data. Many LLMs are trained on data that has been gathered from the Internet — thousands or millions of gigabytes\\' worth of text. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM\\'s programmers may use a more curated data set. [...] A large language model (LLM) is a type of [artificial intelligence (AI)](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/) program that can recognize and generate text, among other tasks. LLMs are trained on [huge sets of data](https://www.cloudflare.com/learning/ai/big-data/) — hence the name \"large.\" LLMs are built on [machine learning](https://www.cloudflare.com/learning/ai/what-is-machine-learning/): specifically, a type of [neural [...] Large language models (LLMs) are machine learning models that can comprehend and generate human language text. They work by analyzing massive data sets of language.\\n\\n##### Learning Center\\n\\n*   [What is artificial intelligence (AI)?](https://www.cloudflare.com/learning/ai/what-is-artificial-intelligence/)\\n    \\n*   [What is a large language model (LLM)?](https://www.cloudflare.com/learning/ai/what-is-large-language-model/)\\n    \\n*   Machine learning\\n*   Glossary\\n\\n#### Learning Objectives'}]\n",
            "\u001b[00m\n",
            "\u001b[31m──\u001b[0m\u001b[31m────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────────\u001b[0m\u001b[31m──\u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mexec_code.py\u001b[0m: \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m128\u001b[0m in \u001b[92mexec_func_with_error_handling\u001b[0m                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mscript_runner\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m669\u001b[0m in \u001b[92mcode_to_exec\u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m263\u001b[0m in \u001b[92m<module>\u001b[0m                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m260 \u001b[0m\u001b[94mif\u001b[0m st.button(\u001b[33m\"\u001b[0m\u001b[33mGet Answer\u001b[0m\u001b[33m\"\u001b[0m):                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m user_question:                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m262 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m st.spinner(\u001b[33m\"\u001b[0m\u001b[33mProcessing your question...\u001b[0m\u001b[33m\"\u001b[0m):                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m263 \u001b[2m│   │   │   \u001b[0manswer = \u001b[1;4mrun_crewai_process(user_question)\u001b[0m                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m264 \u001b[0m\u001b[2m│   │   │   \u001b[0mst.subheader(\u001b[33m\"\u001b[0m\u001b[33mAnswer:\u001b[0m\u001b[33m\"\u001b[0m)                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m265 \u001b[0m\u001b[2m│   │   │   \u001b[0mst.write(answer)                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m266 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m247\u001b[0m in \u001b[92mrun_crewai_process\u001b[0m                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m244 \u001b[0m\u001b[2m│   \u001b[0m)                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m245 \u001b[0m\u001b[2m│   \u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m246 \u001b[0m\u001b[2m│   \u001b[0minputs = {\u001b[33m\"\u001b[0m\u001b[33mquestion\u001b[0m\u001b[33m\"\u001b[0m: question}                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m247 \u001b[2m│   \u001b[0mresult = \u001b[1;4mrag_crew.kickoff(inputs=inputs)\u001b[0m                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m248 \u001b[0m\u001b[2m│   \u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m249 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Clean up the temporary PDF file\u001b[0m                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m250 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m os.path.exists(pdf_path):                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/\u001b[0m\u001b[1;33mcrew.py\u001b[0m:\u001b[94m252\u001b[0m in \u001b[92mkickoff\u001b[0m                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m249 \u001b[0m\u001b[2m│   │   \u001b[0mmetrics = []                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m250 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m251 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.process == Process.sequential:                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m252 \u001b[2m│   │   │   \u001b[0mresult = \u001b[1;4;96mself\u001b[0m\u001b[1;4m._run_sequential_process()\u001b[0m                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m253 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.process == Process.hierarchical:                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m254 \u001b[0m\u001b[2m│   │   │   \u001b[0mresult, manager_metrics = \u001b[96mself\u001b[0m._run_hierarchical_process()         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m255 \u001b[0m\u001b[2m│   │   │   \u001b[0mmetrics.append(manager_metrics)                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/\u001b[0m\u001b[1;33mcrew.py\u001b[0m:\u001b[94m293\u001b[0m in                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_run_sequential_process\u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0magent=role, task=task.description, status=\u001b[33m\"\u001b[0m\u001b[33mstarted\u001b[0m\u001b[33m\"\u001b[0m        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m292 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m293 \u001b[2m│   │   │   \u001b[0moutput = \u001b[1;4mtask.execute(context=task_output)\u001b[0m                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m294 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m task.async_execution:                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m295 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtask_output = output                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m296 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/\u001b[0m\u001b[1;33mtask.py\u001b[0m:\u001b[94m173\u001b[0m in \u001b[92mexecute\u001b[0m                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.thread.start()                                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m173 \u001b[2m│   │   │   \u001b[0mresult = \u001b[96mself\u001b[0m._execute(                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtask=\u001b[96mself\u001b[0m,                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   │   │   │   \u001b[0magent=agent,                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m176 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcontext=context,                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/\u001b[0m\u001b[1;33mtask.py\u001b[0m:\u001b[94m182\u001b[0m in \u001b[92m_execute\u001b[0m               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m179 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m result                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m180 \u001b[0m\u001b[2m│   \u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m181 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_execute\u001b[0m(\u001b[96mself\u001b[0m, agent, task, context, tools):                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m182 \u001b[2m│   │   \u001b[0mresult = \u001b[1;4magent.execute_task(\u001b[0m                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m183 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mtask=task,\u001b[0m                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m184 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mcontext=context,\u001b[0m                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m185 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mtools=tools,\u001b[0m                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/\u001b[0m\u001b[1;33magent.py\u001b[0m:\u001b[94m221\u001b[0m in \u001b[92mexecute_task\u001b[0m          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m218 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.agent_executor.tools_description = render_text_description(parsed \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m219 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.agent_executor.tools_names = \u001b[96mself\u001b[0m.__tools_names(parsed_tools)     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m221 \u001b[2m│   │   \u001b[0mresult = \u001b[96mself\u001b[0m.agent_executor.invoke(                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m│   │   │   \u001b[0m{                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m223 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33minput\u001b[0m\u001b[33m\"\u001b[0m: task_prompt,                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m224 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mtool_names\u001b[0m\u001b[33m\"\u001b[0m: \u001b[96mself\u001b[0m.agent_executor.tools_names,                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain/chains/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m163\u001b[0m in \u001b[92minvoke\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m160 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m161 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mBaseException\u001b[0m \u001b[94mas\u001b[0m e:                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0mrun_manager.on_chain_error(e)                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m163 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m e\u001b[0m                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0mrun_manager.on_chain_end(outputs)                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m165 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m include_run_info:                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain/chains/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m153\u001b[0m in \u001b[92minvoke\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m150 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._validate_inputs(inputs)                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   │   \u001b[0moutputs = (                                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m153 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._call(inputs, run_manager=run_manager)                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m154 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m new_arg_supported                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m155 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m \u001b[96mself\u001b[0m._call(inputs)                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/agents/\u001b[0m\u001b[1;33mexecutor.py\u001b[0m:\u001b[94m124\u001b[0m in \u001b[92m_call\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# We now enter the agent loop (until it returns something).\u001b[0m            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[96mself\u001b[0m._should_continue(\u001b[96mself\u001b[0m.iterations, time_elapsed):            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m123 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m.request_within_rpm_limit \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.request_within_rpm_li \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m124 \u001b[2m│   │   │   │   \u001b[0mnext_step_output = \u001b[96mself\u001b[0m._take_next_step(                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mname_to_tool_map,                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcolor_mapping,                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m127 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0minputs,                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain/agents/\u001b[0m\u001b[1;33magent.py\u001b[0m:\u001b[94m1138\u001b[0m in            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_take_next_step\u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1135 \u001b[0m\u001b[2m│   │   \u001b[0mrun_manager: Optional[CallbackManagerForChainRun] = \u001b[94mNone\u001b[0m,             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1136 \u001b[0m\u001b[2m│   \u001b[0m) -> Union[AgentFinish, List[Tuple[AgentAction, \u001b[96mstr\u001b[0m]]]:                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1137 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._consume_next_step(                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1138 \u001b[2m│   │   │   \u001b[0m[                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1139 \u001b[0m\u001b[2m│   │   │   │   \u001b[0ma                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1140 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m a \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._iter_next_step(                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1141 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mname_to_tool_map,                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain/agents/\u001b[0m\u001b[1;33magent.py\u001b[0m:\u001b[94m1138\u001b[0m in \u001b[92m<listcomp>\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1135 \u001b[0m\u001b[2m│   │   \u001b[0mrun_manager: Optional[CallbackManagerForChainRun] = \u001b[94mNone\u001b[0m,             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1136 \u001b[0m\u001b[2m│   \u001b[0m) -> Union[AgentFinish, List[Tuple[AgentAction, \u001b[96mstr\u001b[0m]]]:                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1137 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._consume_next_step(                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1138 \u001b[2m│   │   │   \u001b[0m[                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1139 \u001b[0m\u001b[2m│   │   │   │   \u001b[0ma                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1140 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m a \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._iter_next_step(                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1141 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mname_to_tool_map,                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/crewai/agents/\u001b[0m\u001b[1;33mexecutor.py\u001b[0m:\u001b[94m186\u001b[0m in             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_iter_next_step\u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m│   │   │   \u001b[0mintermediate_steps = \u001b[96mself\u001b[0m._prepare_intermediate_steps(intermediate \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m184 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m185 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Call the LLM to see what to do.\u001b[0m                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m186 \u001b[2m│   │   │   \u001b[0moutput = \u001b[96mself\u001b[0m.agent.plan(                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m187 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mintermediate_steps,                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcallbacks=run_manager.get_child() \u001b[94mif\u001b[0m run_manager \u001b[94melse\u001b[0m \u001b[94mNone\u001b[0m,    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**inputs,                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain/agents/\u001b[0m\u001b[1;33magent.py\u001b[0m:\u001b[94m397\u001b[0m in \u001b[92mplan\u001b[0m        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 394 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# when using stream_log with the Agent Executor.\u001b[0m                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 395 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Because the response from the plan is not a generator, we need \u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 396 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# accumulate the output into final output and return that.\u001b[0m        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 397 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mfor\u001b[0m\u001b[1;4m chunk \u001b[0m\u001b[1;4;95min\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.runnable.stream(inputs, config={\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mcallbacks\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m: ca\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 398 \u001b[0m\u001b[1;2;4m│   │   │   │   \u001b[0m\u001b[1;4;94mif\u001b[0m\u001b[1;4m final_output \u001b[0m\u001b[1;4;95mis\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;94mNone\u001b[0m\u001b[1;4m:\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 399 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4mfinal_output = chunk\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 400 \u001b[0m\u001b[1;2;4m│   │   │   │   \u001b[0m\u001b[1;4;94melse\u001b[0m\u001b[1;4m:\u001b[0m                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m2875\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mstream\u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2872 \u001b[0m\u001b[2m│   │   \u001b[0mconfig: Optional[RunnableConfig] = \u001b[94mNone\u001b[0m,                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2873 \u001b[0m\u001b[2m│   │   \u001b[0m**kwargs: Optional[Any],                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2874 \u001b[0m\u001b[2m│   \u001b[0m) -> Iterator[Output]:                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m2875 \u001b[2m│   │   \u001b[0m\u001b[1;4;94myield from\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.transform(\u001b[0m\u001b[1;4;96miter\u001b[0m\u001b[1;4m([\u001b[0m\u001b[1;4;96minput\u001b[0m\u001b[1;4m]), config, **kwargs)\u001b[0m            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2876 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2877 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92matransform\u001b[0m(                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2878 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m2862\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mtransform\u001b[0m                                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2859 \u001b[0m\u001b[2m│   │   \u001b[0mconfig: Optional[RunnableConfig] = \u001b[94mNone\u001b[0m,                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2860 \u001b[0m\u001b[2m│   │   \u001b[0m**kwargs: Optional[Any],                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2861 \u001b[0m\u001b[2m│   \u001b[0m) -> Iterator[Output]:                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m2862 \u001b[2m│   │   \u001b[0m\u001b[94myield from\u001b[0m \u001b[96mself\u001b[0m._transform_stream_with_config(                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2863 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96minput\u001b[0m,                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2864 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._transform,                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2865 \u001b[0m\u001b[2m│   │   │   \u001b[0mpatch_config(config, run_name=(config \u001b[95mor\u001b[0m {}).get(\u001b[33m\"\u001b[0m\u001b[33mrun_name\u001b[0m\u001b[33m\"\u001b[0m) \u001b[95mor\u001b[0m \u001b[96ms\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m1881\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_transform_stream_with_config\u001b[0m                                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1878 \u001b[0m\u001b[2m│   │   │   \u001b[0miterator = context.run(transformer, input_for_transform, **kwargs \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1879 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1880 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1881 \u001b[2m│   │   │   │   │   \u001b[0mchunk: Output = context.run(\u001b[96mnext\u001b[0m, iterator)  \u001b[2m# type: igno\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1882 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94myield\u001b[0m chunk                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1883 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m final_output_supported:                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1884 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m final_output \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m2826\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_transform\u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2823 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m),                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2824 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2825 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m2826 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mfor\u001b[0m\u001b[1;4m output \u001b[0m\u001b[1;4;95min\u001b[0m\u001b[1;4m final_pipeline:\u001b[0m                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2827 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;94myield\u001b[0m\u001b[1;4m output\u001b[0m                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2828 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m2829 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_atransform\u001b[0m(                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m1282\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mtransform\u001b[0m                                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1279 \u001b[0m\u001b[2m│   │   \u001b[0mfinal: Input                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1280 \u001b[0m\u001b[2m│   │   \u001b[0mgot_first_val = \u001b[94mFalse\u001b[0m                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1281 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1282 \u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m ichunk \u001b[95min\u001b[0m \u001b[96minput\u001b[0m:                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1283 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# The default implementation of transform is to buffer input and\u001b[0m  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1284 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# then call stream.\u001b[0m                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1285 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# It'll attempt to gather all input into a single chunk using\u001b[0m     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m4736\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mtransform\u001b[0m                                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4733 \u001b[0m\u001b[2m│   │   \u001b[0mconfig: Optional[RunnableConfig] = \u001b[94mNone\u001b[0m,                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4734 \u001b[0m\u001b[2m│   │   \u001b[0m**kwargs: Any,                                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4735 \u001b[0m\u001b[2m│   \u001b[0m) -> Iterator[Output]:                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m4736 \u001b[2m│   │   \u001b[0m\u001b[1;4;94myield from\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.bound.transform(\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4737 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;96minput\u001b[0m\u001b[1;4m,\u001b[0m                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4738 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m._merge_configs(config),\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4739 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4m**{**\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.kwargs, **kwargs},\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m1300\u001b[0m in     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mtransform\u001b[0m                                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1297 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mfinal = ichunk                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1298 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1299 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m got_first_val:                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1300 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94myield from\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.stream(final, config, **kwargs)\u001b[0m                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1301 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1302 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94masync\u001b[0m \u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92matransform\u001b[0m(                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1303 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/\u001b[0m\u001b[1;33mchat_models.p\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m249\u001b[0m in \u001b[92mstream\u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m246 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mgenerations=[[generation]] \u001b[94mif\u001b[0m generation \u001b[94melse\u001b[0m []       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m247 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m),                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m248 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m249 \u001b[2m│   │   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m e\u001b[0m                                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m250 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m251 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mrun_manager.on_llm_end(LLMResult(generations=[[generation]]))  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m252 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/\u001b[0m\u001b[1;33mchat_models.p\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m229\u001b[0m in \u001b[92mstream\u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m227 \u001b[0m\u001b[2m│   │   │   \u001b[0mgeneration: Optional[ChatGenerationChunk] = \u001b[94mNone\u001b[0m                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m228 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m229 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m chunk \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._stream(messages, stop=stop, **kwargs):      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m230 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m chunk.message.id \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m231 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mchunk.message.id = \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mrun-\u001b[0m\u001b[33m{\u001b[0mrun_manager.run_id\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m232 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mchunk.message.response_metadata = _gen_info_and_msg_metada \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m480\u001b[0m in  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_stream\u001b[0m                                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 477 \u001b[0m\u001b[2m│   │   \u001b[0mparams = {**params, **kwargs, \u001b[33m\"\u001b[0m\u001b[33mstream\u001b[0m\u001b[33m\"\u001b[0m: \u001b[94mTrue\u001b[0m}                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 478 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 479 \u001b[0m\u001b[2m│   │   \u001b[0mdefault_chunk_class = AIMessageChunk                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 480 \u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m.client.create(messages=message_dicts, **params)\u001b[0m \u001b[94mas\u001b[0m response \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 481 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m chunk \u001b[95min\u001b[0m response:                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 482 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(chunk, \u001b[96mdict\u001b[0m):                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 483 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mchunk = chunk.model_dump()                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/openai/_utils/\u001b[0m\u001b[1;33m_utils.py\u001b[0m:\u001b[94m287\u001b[0m in \u001b[92mwrapper\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m284 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m285 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mmsg = \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mMissing required argument: \u001b[0m\u001b[33m{\u001b[0mquote(missing[\u001b[94m0\u001b[0m])\u001b[33m}\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m286 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mTypeError\u001b[0m(msg)                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m287 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mfunc(*args, **kwargs)\u001b[0m                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m288 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m289 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m wrapper  \u001b[2m# type: ignore\u001b[0m                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m290 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/\u001b[0m\u001b[1;33mcompletion\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33ms.py\u001b[0m:\u001b[94m925\u001b[0m in \u001b[92mcreate\u001b[0m                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 922 \u001b[0m\u001b[2m│   │   \u001b[0mtimeout: \u001b[96mfloat\u001b[0m | httpx.Timeout | \u001b[94mNone\u001b[0m | NotGiven = NOT_GIVEN,         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 923 \u001b[0m\u001b[2m│   \u001b[0m) -> ChatCompletion | Stream[ChatCompletionChunk]:                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 924 \u001b[0m\u001b[2m│   │   \u001b[0mvalidate_response_format(response_format)                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 925 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._post(                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 926 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m/chat/completions\u001b[0m\u001b[33m\"\u001b[0m,                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 927 \u001b[0m\u001b[2m│   │   │   \u001b[0mbody=maybe_transform(                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 928 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m{                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/openai/\u001b[0m\u001b[1;33m_base_client.py\u001b[0m:\u001b[94m1242\u001b[0m in \u001b[92mpost\u001b[0m          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1239 \u001b[0m\u001b[2m│   │   \u001b[0mopts = FinalRequestOptions.construct(                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1240 \u001b[0m\u001b[2m│   │   │   \u001b[0mmethod=\u001b[33m\"\u001b[0m\u001b[33mpost\u001b[0m\u001b[33m\"\u001b[0m, url=path, json_data=body, files=to_httpx_files(fil \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1241 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1242 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m cast(ResponseT, \u001b[1;4;96mself\u001b[0m\u001b[1;4m.request(cast_to, opts, stream=stream, str\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1243 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1244 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mpatch\u001b[0m(                                                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1245 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/openai/\u001b[0m\u001b[1;33m_base_client.py\u001b[0m:\u001b[94m1037\u001b[0m in \u001b[92mrequest\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1034 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0merr.response.read()                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1035 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1036 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlog.debug(\u001b[33m\"\u001b[0m\u001b[33mRe-raising status error\u001b[0m\u001b[33m\"\u001b[0m)                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1037 \u001b[2m│   │   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m._make_status_error_from_response(err.response) \u001b[0m\u001b[1;4;94mfro\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1038 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1039 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1040 \u001b[0m                                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[1;91mAPIStatusError: \u001b[0mError code: \u001b[1;36m413\u001b[0m - \u001b[1m{\u001b[0m\u001b[32m'error'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'message'\u001b[0m: \u001b[32m'Request too large for model \u001b[0m\n",
            "\u001b[32m`llama3-8b-8192` in organization `org_01jv986wb6e4esj2pz6rqtmahc` service tier \u001b[0m\n",
            "\u001b[32m`on_demand` on tokens per minute \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTPM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: Limit 6000, Requested 6768, please reduce your \u001b[0m\n",
            "\u001b[32mmessage size and try again. Need more tokens? Upgrade to Dev Tier today at \u001b[0m\n",
            "\u001b[32mhttps://console.groq.com/settings/billing'\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'tokens'\u001b[0m, \u001b[32m'code'\u001b[0m: \n",
            "\u001b[32m'rate_limit_exceeded'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n",
            "/tools/node/lib/node_modules/localtunnel/bin/lt.js:81\n",
            "    throw err;\n",
            "    ^\n",
            "\n",
            "Error: connection refused: localtunnel.me:5297 (check your firewall settings)\n",
            "    at Socket.<anonymous> (/tools/node/lib/node_modules/\u001b[4mlocaltunnel\u001b[24m/lib/TunnelCluster.js:52:11)\n",
            "\u001b[90m    at Socket.emit (node:events:524:28)\u001b[39m\n",
            "\u001b[90m    at emitErrorNT (node:internal/streams/destroy:169:8)\u001b[39m\n",
            "\u001b[90m    at emitErrorCloseNT (node:internal/streams/destroy:128:3)\u001b[39m\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)\u001b[39m\n",
            "\n",
            "Node.js v20.19.0\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0128a1409279403380b56c2fcf9e6b3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03b73beb36964d7e94137f6c7f1e9bde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04ed34ef85714be9b1ea0d0ef5609307": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0525060f77bb4a86b203c4f734ae7514": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9b0a2d3bfe9491086bd9a27ad8bf0ca",
              "IPY_MODEL_874ce4d74a2d43e0802c33aa299b869c",
              "IPY_MODEL_4a510dc710c144078487f8364b8da607"
            ],
            "layout": "IPY_MODEL_7dabe8903cca47ff85a0c80b42805959"
          }
        },
        "06d2a9b91d5c4041afe2803cf2130fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e231320e3d22411cb844e2f69e84f7c2",
            "placeholder": "​",
            "style": "IPY_MODEL_7227f155638e4e408123543600c0d8e0",
            "value": "config.json: 100%"
          }
        },
        "0982a39041864c918fb2e53d4e4eed9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0dd08f40bbd240eeb26bb4f1f7d95953": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e24f115576f4a9d854b52235b7d1100": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ca5c187aff445f782f1ecebdb949b4c",
            "placeholder": "​",
            "style": "IPY_MODEL_9e1ff8af6625440988d5d2f666da124e",
            "value": " 743/743 [00:00&lt;00:00, 48.5kB/s]"
          }
        },
        "11f607cda74444a99cd24e9bcc569d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dd08f40bbd240eeb26bb4f1f7d95953",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b541ae03c644cb9954e5b94b898708e",
            "value": 231508
          }
        },
        "13665a13362a45c3be8eac43fc647252": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a5f3b8988a24f91a2578dead195ee71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cf787e298e444a4a03260b99421dd8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e17d321123249839c4cf67c4d62bef4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f59e863e66b4123b40d122d56a79e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f65f2fde8c44260b4841c06f0f6aa27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22ccf6291cfa431d98f033d0ce435ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3f8b5143e8541ff968e418779b9534e",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cba8850fda6e4f50ae09dbb0c09b8851",
            "value": 743
          }
        },
        "231c4983cba64f02bd8bd71c153b16ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23ab8983b9a141218fece2b98997f080": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23ad88c54efc4ec59e91e187b0108fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e17d321123249839c4cf67c4d62bef4",
            "placeholder": "​",
            "style": "IPY_MODEL_fbc1e4d4f3554510bf037cf9a741baba",
            "value": " 125/125 [00:00&lt;00:00, 11.6kB/s]"
          }
        },
        "2438fec919d74b7bab0e998553343dc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29de276a1b014357bd063f1785747e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae5642c1bb00441ea62cde365bdb4bc1",
            "placeholder": "​",
            "style": "IPY_MODEL_1f65f2fde8c44260b4841c06f0f6aa27",
            "value": " 124/124 [00:00&lt;00:00, 5.18kB/s]"
          }
        },
        "2ab9fae4cc624dc6b59e5d3f094e8240": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ae7b3732ea644cc9df927b5d959ccdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06d2a9b91d5c4041afe2803cf2130fe4",
              "IPY_MODEL_22ccf6291cfa431d98f033d0ce435ea4",
              "IPY_MODEL_0e24f115576f4a9d854b52235b7d1100"
            ],
            "layout": "IPY_MODEL_23ab8983b9a141218fece2b98997f080"
          }
        },
        "2b3e5cfc9d3941bb8cbcf1c60c0639a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c9a907d37f644228b6a358c11d9730c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_413f645d3e9f438da201f081b017d212",
            "placeholder": "​",
            "style": "IPY_MODEL_c4dd7245a9854a0d99bd18e84e02455e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "2f9761e9909447d6bf6f696d50c1cf71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36abad11fbd844b48e636be8cd3b55f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9095db6d7dc42e09a46bc54d491f3b4",
            "placeholder": "​",
            "style": "IPY_MODEL_491cdfb8894b469487dd4a9511bf3692",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3ba90d833ffc42f0891749f5de251de6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dc5e10aa5454133950b30f9de5bc4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0128a1409279403380b56c2fcf9e6b3d",
            "max": 94783,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0982a39041864c918fb2e53d4e4eed9c",
            "value": 94783
          }
        },
        "408ce81d723c4912b98a94fa28c8c00a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "413f645d3e9f438da201f081b017d212": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "416f584aabab4bbcb52c32c79b0518a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45d76c07213f44ee908ca7f8d568f3fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "491cdfb8894b469487dd4a9511bf3692": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4958f60ebba5459986ce18f5693e8e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a510dc710c144078487f8364b8da607": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a5f3b8988a24f91a2578dead195ee71",
            "placeholder": "​",
            "style": "IPY_MODEL_bead3fd07582495081888b97050abf81",
            "value": " 52.0/52.0 [00:00&lt;00:00, 2.59kB/s]"
          }
        },
        "51e00c81c4a64e1ab97380880d660bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5212e027c9ec43bf85cff3731c2232f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "522e77b34dae4521b7679cd2eafcb39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f9761e9909447d6bf6f696d50c1cf71",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3e9de2fac82416ca2ebc468d30882ae",
            "value": 366
          }
        },
        "528103f63c11499b801cae916e94e08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c82050c17dd945dca5e55f09a7ff370b",
              "IPY_MODEL_3dc5e10aa5454133950b30f9de5bc4f1",
              "IPY_MODEL_6f0d685710c74f839732d67b0aa83101"
            ],
            "layout": "IPY_MODEL_f9fd06ebabd34ec9afd99ad6c1f18bb8"
          }
        },
        "5312837f7c3f456ea871b39939e542b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54b11857f50149cfabb02739ae4d7411": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b541ae03c644cb9954e5b94b898708e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bb0396eb0be4b9bbcdd7ebb86b6bbf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5b69528d90462cb4cfd76ecbbeccc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ec27e38471e4411aeb6bb3864d7339a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "615b7a70286e41519031db0fec9c52dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61cee338a8e0416db158abf9ec6ae59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "673ee16e85fa4d6cbf519d6baaffff16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_408ce81d723c4912b98a94fa28c8c00a",
            "placeholder": "​",
            "style": "IPY_MODEL_d293045b68ef4c309ee7029f870e57b4",
            "value": " 232k/232k [00:00&lt;00:00, 8.51MB/s]"
          }
        },
        "69decfc9528d41489d5cde1c37a98445": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92bd9de3d40440ed94957d646a0cabbf",
              "IPY_MODEL_87219cd862034305903a4530c7fb847b",
              "IPY_MODEL_d72a6036f5b34e74b9074b4e2d05f071"
            ],
            "layout": "IPY_MODEL_c86d1c424ee944c4a969def772aced42"
          }
        },
        "6a40dd8858db4756ba0176ff51bf5629": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a436c3307fc409893697bf95470e2ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a779c5455d842759ccdfb5767fbafed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6be655ccd73246038f6ce632cd67901d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e640cfc2b064b48ab91cda5a5c8c8c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2438fec919d74b7bab0e998553343dc9",
            "placeholder": "​",
            "style": "IPY_MODEL_6a40dd8858db4756ba0176ff51bf5629",
            "value": " 349/349 [00:00&lt;00:00, 17.6kB/s]"
          }
        },
        "6f0d685710c74f839732d67b0aa83101": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f077f4c82c541fdad48218483381fc3",
            "placeholder": "​",
            "style": "IPY_MODEL_1f59e863e66b4123b40d122d56a79e59",
            "value": " 94.8k/94.8k [00:00&lt;00:00, 3.21MB/s]"
          }
        },
        "719a1fe494864ff3979ac996a8e621e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5212e027c9ec43bf85cff3731c2232f2",
            "placeholder": "​",
            "style": "IPY_MODEL_2ab9fae4cc624dc6b59e5d3f094e8240",
            "value": " 711k/711k [00:00&lt;00:00, 26.7MB/s]"
          }
        },
        "7227f155638e4e408123543600c0d8e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7acfd96f99a34c2c9932629fbcd3c279": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_416f584aabab4bbcb52c32c79b0518a8",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a34c09d3460e48cb8d822f10d7111f56",
            "value": 125
          }
        },
        "7c5da6920a2848679ea70ec4f6dee3f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab2290afa97c4a4e817f8ab928442204",
            "placeholder": "​",
            "style": "IPY_MODEL_e1696d73397a4992bdfc013d18d5c2d0",
            "value": "modules.json: 100%"
          }
        },
        "7ca5c187aff445f782f1ecebdb949b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dabe8903cca47ff85a0c80b42805959": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fb162ec6a844167962c7280e84756b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3e5cfc9d3941bb8cbcf1c60c0639a5",
            "placeholder": "​",
            "style": "IPY_MODEL_45d76c07213f44ee908ca7f8d568f3fb",
            "value": "model.safetensors: 100%"
          }
        },
        "812eea4e4dbe4000b82dc3c6641b5a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a436c3307fc409893697bf95470e2ad",
            "placeholder": "​",
            "style": "IPY_MODEL_5e5b69528d90462cb4cfd76ecbbeccc6",
            "value": " 366/366 [00:00&lt;00:00, 31.2kB/s]"
          }
        },
        "8229e26f67ab4b8dac08d4fed383e812": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88d3b2f4311e42418d47a5e7681f9926",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a779c5455d842759ccdfb5767fbafed",
            "value": 349
          }
        },
        "87219cd862034305903a4530c7fb847b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baf9ea586b894122962d984ec5facf85",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61cee338a8e0416db158abf9ec6ae59f",
            "value": 190
          }
        },
        "874ce4d74a2d43e0802c33aa299b869c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5312837f7c3f456ea871b39939e542b4",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4bcf8943bb34d8a9403548ebff58d12",
            "value": 52
          }
        },
        "87a45fbe66d44c22ab3c96cfa107870d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0602b8a19aa4a70952c7559136e86a9",
            "placeholder": "​",
            "style": "IPY_MODEL_e44cec9420dc40d0866ec7216f2e0e5f",
            "value": " 133M/133M [00:01&lt;00:00, 129MB/s]"
          }
        },
        "8853fd1d51624303a747cdd0843cb6c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88d3b2f4311e42418d47a5e7681f9926": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "897eb7d5f8384f6987ede3b704444526": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bc8386d4ce04f628cad98b7ca1582ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f077f4c82c541fdad48218483381fc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92bd9de3d40440ed94957d646a0cabbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13665a13362a45c3be8eac43fc647252",
            "placeholder": "​",
            "style": "IPY_MODEL_c300296649e24fb2ad93aa563e4ee7e4",
            "value": "config.json: 100%"
          }
        },
        "9e1ff8af6625440988d5d2f666da124e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a34c09d3460e48cb8d822f10d7111f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4c53d6aaf43422e8eb8540c421bc3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e41f9801bf56479f8cfa4ed9be9ef88f",
              "IPY_MODEL_adb5c749685a4d50be3cf2bb22128834",
              "IPY_MODEL_719a1fe494864ff3979ac996a8e621e4"
            ],
            "layout": "IPY_MODEL_c6ebf6d3924f42a1b8d41f92039443cf"
          }
        },
        "a683056f96c5452fb94a4e41f0401e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daa06c7413d340e8935024e9cb747d14",
            "placeholder": "​",
            "style": "IPY_MODEL_de023d1cc1e24bd9a63e289225df2729",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "a6ddf1c114334e1f9bdf1b7feb3baae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a683056f96c5452fb94a4e41f0401e63",
              "IPY_MODEL_d46ed19d45124fb2a84f065a783ac2ac",
              "IPY_MODEL_29de276a1b014357bd063f1785747e46"
            ],
            "layout": "IPY_MODEL_8bc8386d4ce04f628cad98b7ca1582ab"
          }
        },
        "ab2290afa97c4a4e817f8ab928442204": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acf1e401fa464763820ed27bb37c9e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c9a907d37f644228b6a358c11d9730c",
              "IPY_MODEL_7acfd96f99a34c2c9932629fbcd3c279",
              "IPY_MODEL_23ad88c54efc4ec59e91e187b0108fea"
            ],
            "layout": "IPY_MODEL_03b73beb36964d7e94137f6c7f1e9bde"
          }
        },
        "adb5c749685a4d50be3cf2bb22128834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_231c4983cba64f02bd8bd71c153b16ac",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9014aed7d054555ab8551ba015c7d57",
            "value": 711396
          }
        },
        "ae5642c1bb00441ea62cde365bdb4bc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0602b8a19aa4a70952c7559136e86a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1f47123d6704e80bbc87ae5c05b8304": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3721d2aeaba4f34a5302f8b0557ffba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5d76a566374430288e02b528095cac3",
              "IPY_MODEL_11f607cda74444a99cd24e9bcc569d5b",
              "IPY_MODEL_673ee16e85fa4d6cbf519d6baaffff16"
            ],
            "layout": "IPY_MODEL_1cf787e298e444a4a03260b99421dd8a"
          }
        },
        "b4e63e9e63a14238b9e452ea0ba94a47": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b581dad444fe4a63a7a674ab47c0e826": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36abad11fbd844b48e636be8cd3b55f8",
              "IPY_MODEL_522e77b34dae4521b7679cd2eafcb39b",
              "IPY_MODEL_812eea4e4dbe4000b82dc3c6641b5a73"
            ],
            "layout": "IPY_MODEL_51e00c81c4a64e1ab97380880d660bc0"
          }
        },
        "baf9ea586b894122962d984ec5facf85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bead3fd07582495081888b97050abf81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf3b0d9ab2974d2583e893ed50cbf880": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c300296649e24fb2ad93aa563e4ee7e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4dd7245a9854a0d99bd18e84e02455e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5ffd4c355494bd686a713cf46e0c0f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6ebf6d3924f42a1b8d41f92039443cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c82050c17dd945dca5e55f09a7ff370b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_897eb7d5f8384f6987ede3b704444526",
            "placeholder": "​",
            "style": "IPY_MODEL_04ed34ef85714be9b1ea0d0ef5609307",
            "value": "README.md: 100%"
          }
        },
        "c86d1c424ee944c4a969def772aced42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9095db6d7dc42e09a46bc54d491f3b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c935e4fbac764774aee404f844b7c1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fb162ec6a844167962c7280e84756b6",
              "IPY_MODEL_ee0b4d7898ef4bb697b44d000ebb5fd7",
              "IPY_MODEL_87a45fbe66d44c22ab3c96cfa107870d"
            ],
            "layout": "IPY_MODEL_54b11857f50149cfabb02739ae4d7411"
          }
        },
        "cba8850fda6e4f50ae09dbb0c09b8851": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc89f129fd4d410ab3686eb9b96adb11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd4fc4eb2c64090868ac9effe2843aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c5da6920a2848679ea70ec4f6dee3f4",
              "IPY_MODEL_8229e26f67ab4b8dac08d4fed383e812",
              "IPY_MODEL_6e640cfc2b064b48ab91cda5a5c8c8c1"
            ],
            "layout": "IPY_MODEL_f304b3d7f8e64784af92e99a3b437b54"
          }
        },
        "d293045b68ef4c309ee7029f870e57b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d46ed19d45124fb2a84f065a783ac2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4e63e9e63a14238b9e452ea0ba94a47",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf3b0d9ab2974d2583e893ed50cbf880",
            "value": 124
          }
        },
        "d4bcf8943bb34d8a9403548ebff58d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d72a6036f5b34e74b9074b4e2d05f071": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6be655ccd73246038f6ce632cd67901d",
            "placeholder": "​",
            "style": "IPY_MODEL_615b7a70286e41519031db0fec9c52dd",
            "value": " 190/190 [00:00&lt;00:00, 12.5kB/s]"
          }
        },
        "d9014aed7d054555ab8551ba015c7d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9b0a2d3bfe9491086bd9a27ad8bf0ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1f47123d6704e80bbc87ae5c05b8304",
            "placeholder": "​",
            "style": "IPY_MODEL_8853fd1d51624303a747cdd0843cb6c1",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "daa06c7413d340e8935024e9cb747d14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de023d1cc1e24bd9a63e289225df2729": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1696d73397a4992bdfc013d18d5c2d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e231320e3d22411cb844e2f69e84f7c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3f8b5143e8541ff968e418779b9534e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e41f9801bf56479f8cfa4ed9be9ef88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bb0396eb0be4b9bbcdd7ebb86b6bbf6",
            "placeholder": "​",
            "style": "IPY_MODEL_4958f60ebba5459986ce18f5693e8e8d",
            "value": "tokenizer.json: 100%"
          }
        },
        "e44cec9420dc40d0866ec7216f2e0e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee0b4d7898ef4bb697b44d000ebb5fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc89f129fd4d410ab3686eb9b96adb11",
            "max": 133466304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5ffd4c355494bd686a713cf46e0c0f2",
            "value": 133466304
          }
        },
        "f304b3d7f8e64784af92e99a3b437b54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3e9de2fac82416ca2ebc468d30882ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5d76a566374430288e02b528095cac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ba90d833ffc42f0891749f5de251de6",
            "placeholder": "​",
            "style": "IPY_MODEL_5ec27e38471e4411aeb6bb3864d7339a",
            "value": "vocab.txt: 100%"
          }
        },
        "f9fd06ebabd34ec9afd99ad6c1f18bb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbc1e4d4f3554510bf037cf9a741baba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}